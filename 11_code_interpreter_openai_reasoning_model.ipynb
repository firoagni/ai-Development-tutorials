{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b317bfe0",
   "metadata": {},
   "source": [
    "# Code Interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ce0ec",
   "metadata": {},
   "source": [
    "Traditional LLMs are good at generating text, but they struggle with tasks that require maths or calculations.\n",
    "<br><br>\n",
    "**Example:** How many \"r\"s present in the string \"strawberry\"?<br>\n",
    "**Answer from LLM:** \"strawberry\" has 2 \"r\"s.\n",
    "<br><br>\n",
    "Yikes!\n",
    "<br><br>\n",
    "Discussions regarding LLMs canâ€™t count:\n",
    "- https://community.openai.com/t/should-a-custom-gpt-be-able-to-count-the-number-of-items-in-a-json-list/575999\u000bhttps://community.openai.com/t/assistant-can-not-search-the-whole-file-using-file-search/739661/3\n",
    "- https://www.reddit.com/r/OpenAI/comments/15xfcuk/how_do_i_pass_complex_and_nested_large_json_data\n",
    "\n",
    "<br>\n",
    "To solve this problem, OpenAI has introduced a feature called \"Code Interpreter\"\n",
    "\n",
    "- Code Interpreter allows `Assistants` to write and run Python code in a sandboxed execution environment.\n",
    "- If the generated code fails to execute, the Assistant will iteratively debug and refine the code until the code executes successfully.\n",
    "With Code Interpreter enabled, your Assistant can now solve code, math, and data analysis problems.\n",
    "\n",
    "### Steps:\n",
    "1. Upload a file (CSV, JSON, etc.) to Azure Server.\n",
    "1. Create an `assistant` using `assistant API` and provide it access to the file\n",
    "1. Create a `thread` for the `assistant` with the purpose of analyzing the file and providing results based on the given instructions.\n",
    "1. The `assistant` will generate and run a Python code to analyze the file.\n",
    "    - The analysis results will be dumped to a file\n",
    "    - Once the thread execution is completed, the Assistant will return the results.\n",
    "1. Print the results\n",
    "1. Delete the uploaded file from the Azure Server.\n",
    "\n",
    "### References\n",
    "- https://platform.openai.com/docs/assistants/tools/code-interpreter\n",
    "- https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/code-interpreter?tabs=python\n",
    "- https://platform.openai.com/docs/assistants/quickstart?example=without-streaming\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf7a8fc",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Make sure that `python3` is installed on your system.\n",
    "1. Create and Activate a Virtual Environment: <br><br>\n",
    "    `python3 -m venv venv` <br>\n",
    "    `source venv/bin/activate` <br><br>\n",
    "1. Create a `.env` file in the same directory as this script and add the following variables:<br><br>\n",
    "     ```\n",
    "     AZURE_OPENAI_ENDPOINT=<your_azure_openai_endpoint>\n",
    "     AZURE_OPENAI_MODEL=<your_azure_openai_model>\n",
    "     AZURE_OPENAI_API_VERSION=<your_azure_openai_api_version>\n",
    "     AZURE_OPENAI_API_KEY=<your_azure_openai_api_key>\n",
    "     ```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9c043",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "The required libraries are listed in the requirements.txt file. Use the following command to install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bdacd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: openai in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (1.78.0)\n",
      "Requirement already satisfied: dotenv in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (0.9.9)\n",
      "Requirement already satisfied: langchain in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (0.3.25)\n",
      "Requirement already satisfied: langchain-openai in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 5)) (0.3.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (2025.4.26)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 2)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 2)) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 2)) (2.11.4)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 2)) (4.13.2)\n",
      "Requirement already satisfied: python-dotenv in ./venv/lib/python3.13/site-packages (from dotenv->-r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in ./venv/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 4)) (0.3.59)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in ./venv/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 4)) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in ./venv/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 4)) (0.3.42)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./venv/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 4)) (2.0.40)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./venv/lib/python3.13/site-packages (from langchain->-r requirements.txt (line 4)) (6.0.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in ./venv/lib/python3.13/site-packages (from langchain-openai->-r requirements.txt (line 5)) (0.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (0.16.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 4)) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 4)) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 4)) (24.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 4)) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 4)) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./venv/lib/python3.13/site-packages (from tiktoken<1,>=0.7->langchain-openai->-r requirements.txt (line 5)) (2024.11.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./venv/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 4)) (3.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f81db44",
   "metadata": {},
   "source": [
    "***\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3644f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI  # The `AzureOpenAI` library is used to interact with the Azure OpenAI API.\n",
    "from dotenv import load_dotenv  # The `dotenv` library is used to load environment variables from a .env file.\n",
    "import os                       # Used to get the values from environment variables.\n",
    "import json                     # The `json` library is used to work with JSON data in Python.\n",
    "from pprint import pprint       # The `pprint` library is used to pretty-print a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d7dfa8",
   "metadata": {},
   "source": [
    "## Load environment variables from .env file\n",
    "\n",
    "The `load_dotenv()` function reads the .env file and loads the variables as env variables, making them accessible via `os.environ` or `os.getenv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52016dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT        = os.environ['AZURE_OPENAI_ENDPOINT']\n",
    "AZURE_OPENAI_MODEL           = os.environ['AZURE_OPENAI_MODEL']\n",
    "AZURE_OPENAI_API_VERSION     = os.environ['AZURE_OPENAI_VERSION']\n",
    "AZURE_OPENAI_API_KEY         = os.environ['AZURE_OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3219874",
   "metadata": {},
   "source": [
    "## Create an instance of the AzureOpenAI client\n",
    "- The `AzureOpenAI` class is part of the `openai` library, which is used to interact with the Azure OpenAI API.\n",
    "- It requires the Azure endpoint, API key, and API version to be passed as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29817216",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    azure_endpoint = AZURE_OPENAI_ENDPOINT,\n",
    "    api_key = AZURE_OPENAI_API_KEY,  \n",
    "    api_version = AZURE_OPENAI_API_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da423e4c",
   "metadata": {},
   "source": [
    "## Step 1: Upload your file to Azure Server with an \"assistants\" purpose\n",
    "\n",
    "What is a `purpose`?<br>\n",
    "When you upload a file to Azure OpenAI, you need to specify the purpose of the file.\n",
    "<br>\n",
    "The following purposes are supported:\n",
    "https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/code-interpreter?tabs=python#supported-file-types\n",
    "<br><br>\n",
    "What file formats are supported for upload?<br>\n",
    "https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/code-interpreter?tabs=python#supported-file-types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a447eced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file, file ID: assistant-THMsYwk8Tfbsj8mgr8uLcv\n"
     ]
    }
   ],
   "source": [
    "file = client.files.create(\n",
    "    file=open(\"dummy_build_data.json\", \"rb\"), #multipart file upload requires the file to be in binary not in text\n",
    "    purpose='assistants' \n",
    ")\n",
    "# Use file.id to refer to the file\n",
    "print(f\"Uploaded file, file ID: {file.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0438fe7",
   "metadata": {},
   "source": [
    "Note: You cannot view the content of a file uploaded to the Azure OpenAI server if the purpose is defined as `assistants`\n",
    "<br><br>\n",
    "The following code will not work:\n",
    "```python\n",
    "uploaded_file_content = client.files.content(file.id)\n",
    "```\n",
    "The command will throw the following error:\n",
    "```python\n",
    "openai.error.InvalidRequestError: The file content is not available for the purpose of \"assistants\".\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46479058",
   "metadata": {},
   "source": [
    "## Step 2: Create an \"assistant\" using assistant API \n",
    "Instruct that `code_interpreter` is enabled and provide this assistant access to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec975c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new assistant asst_ILUKBXBTwrrs0TYLNv15mHJF created:\n",
      "\n",
      "Assistant(id='asst_ILUKBXBTwrrs0TYLNv15mHJF', created_at=1747457232, description=None, instructions='You are an AI assistant that can read and analyze JSON files. The JSON file contains Jenkins build information under the key `results`. Each entry in the `results` array contains information about a build. Build status of a build can be found by checking the `build_status` key. Build duration (time build took to complete) can be found by checking the `build_duration` key. Queue time (time build spent in queue) can be found by checking the `queue_time` key. Build label can be found by checking the `build_label` key. When somebody ask about a build, make sure to provide the build label. ', metadata={}, model='gpt-4', name='build-analyzer-agent', object='assistant', tools=[CodeInterpreterTool(type='code_interpreter')], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=ToolResourcesCodeInterpreter(file_ids=['assistant-THMsYwk8Tfbsj8mgr8uLcv']), file_search=None), top_p=1.0)\n"
     ]
    }
   ],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    model=AZURE_OPENAI_MODEL,\n",
    "    name=\"build-analyzer-agent\", # name of the agent (optional)    \n",
    "    instructions=\"You are an AI assistant that can read and analyze JSON files. \"\n",
    "            \"The JSON file contains Jenkins build information under the key `results`. \"\n",
    "            \"Each entry in the `results` array contains information about a build. \"\n",
    "            \"Build status of a build can be found by checking the `build_status` key. \"\n",
    "            \"Build duration (time build took to complete) can be found by checking the `build_duration` key. \"\n",
    "            \"Queue time (time build spent in queue) can be found by checking the `queue_time` key. \"\n",
    "            \"Build label can be found by checking the `build_label` key. When somebody ask about a build, make sure to provide the build label. \",\n",
    "    tools=[{\"type\": \"code_interpreter\"}], # mentions that the assistant can use the code interpreter tool\n",
    "    tool_resources={\"code_interpreter\":{\"file_ids\":[file.id]}} # mentions that the assistant can use the file we just uploaded\n",
    ")\n",
    "print(f\"A new assistant {assistant.id} created:\\n\")\n",
    "print(assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec55d1",
   "metadata": {},
   "source": [
    "## Step 3: Create a thread for the assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59e84944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new thread: thread_OJG6UWDKipgOy9WQNwJm07gD created:\n",
      "\n",
      "Thread(id='thread_OJG6UWDKipgOy9WQNwJm07gD', created_at=1747457233, metadata={}, object='thread', tool_resources=ToolResources(code_interpreter=None, file_search=None))\n"
     ]
    }
   ],
   "source": [
    "thread = client.beta.threads.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Provide Total builds and list all build statuses along their counts and percentages. \"\n",
    "                        \"Also provide the fastest and the slowest build along with their build duration. \"\n",
    "                        \"Also provide the build labels with the longest and shortest queue time. Provide durations too. \"\n",
    "                        \"Also provide the average build and queue duration. \"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(f\"A new thread: {thread.id} created:\\n\")\n",
    "print(thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a07503",
   "metadata": {},
   "source": [
    "## Step 4: Run the thread with the assistant\n",
    "\n",
    "Ones a thread is created, you can \"run\" it with any assistant (in real projects, you may have multiple assistants created for different purposes).\n",
    "<br><br>\n",
    "Also note that thread runs are asynchronous, which means you'll need to monitor their status by polling the Run object until a termination status is reached. \n",
    "<br><br> \n",
    "If however, you are not bothered about streaming, then use the convenience helper method `create_and_poll` that can assist both in creating the run and then polling for its completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dc0e237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running thread: thread_OJG6UWDKipgOy9WQNwJm07gD with assistant: asst_ILUKBXBTwrrs0TYLNv15mHJF...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running thread: {thread.id} with assistant: {assistant.id}...\\n\")\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcefb255",
   "metadata": {},
   "source": [
    "The above code is equivalent to `Thread.run()` in `Java`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9069e890",
   "metadata": {},
   "source": [
    "## Step 5: Capture result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f4bd96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------\n",
      "\n",
      "Thread run completed and returned the following JSON:.\n",
      "\n",
      "{\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"id\": \"msg_4xT0bRGNV5terVf9NDIdsllf\",\n",
      "            \"assistant_id\": null,\n",
      "            \"attachments\": [],\n",
      "            \"completed_at\": null,\n",
      "            \"content\": [\n",
      "                {\n",
      "                    \"text\": {\n",
      "                        \"annotations\": [],\n",
      "                        \"value\": \"Provide Total builds and list all build statuses along their counts and percentages. Also provide the fastest and the slowest build along with their build duration. Also provide the build labels with the longest and shortest queue time. Provide durations too. Also provide the average build and queue duration. \"\n",
      "                    },\n",
      "                    \"type\": \"text\"\n",
      "                }\n",
      "            ],\n",
      "            \"created_at\": 1747457233,\n",
      "            \"incomplete_at\": null,\n",
      "            \"incomplete_details\": null,\n",
      "            \"metadata\": {},\n",
      "            \"object\": \"thread.message\",\n",
      "            \"role\": \"user\",\n",
      "            \"run_id\": null,\n",
      "            \"status\": null,\n",
      "            \"thread_id\": \"thread_OJG6UWDKipgOy9WQNwJm07gD\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"msg_PNoLLQxJbPfGwXoByZNHK6WG\",\n",
      "            \"assistant_id\": \"asst_ILUKBXBTwrrs0TYLNv15mHJF\",\n",
      "            \"attachments\": [],\n",
      "            \"completed_at\": null,\n",
      "            \"content\": [\n",
      "                {\n",
      "                    \"text\": {\n",
      "                        \"annotations\": [],\n",
      "                        \"value\": \"I apologize for the error. There was a type mismatch when calculating the total build duration. It seems like one or more build durations in the JSON data are represented as strings instead of numbers. I will handle this by converting build duration and queue time to integers. Let's try again.\"\n",
      "                    },\n",
      "                    \"type\": \"text\"\n",
      "                }\n",
      "            ],\n",
      "            \"created_at\": 1747457247,\n",
      "            \"incomplete_at\": null,\n",
      "            \"incomplete_details\": null,\n",
      "            \"metadata\": {},\n",
      "            \"object\": \"thread.message\",\n",
      "            \"role\": \"assistant\",\n",
      "            \"run_id\": \"run_2Wy2xp5dMoAqVVrLc82nXKEm\",\n",
      "            \"status\": null,\n",
      "            \"thread_id\": \"thread_OJG6UWDKipgOy9WQNwJm07gD\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"msg_RgplcRMZFggqMeJP2NEqVJd4\",\n",
      "            \"assistant_id\": \"asst_ILUKBXBTwrrs0TYLNv15mHJF\",\n",
      "            \"attachments\": [],\n",
      "            \"completed_at\": null,\n",
      "            \"content\": [\n",
      "                {\n",
      "                    \"text\": {\n",
      "                        \"annotations\": [],\n",
      "                        \"value\": \"I apologize for the confusion. The build_duration and queue_time seem to be in a time format of 'HH:MM:SS.sss'. Let me convert this into seconds to perform the calculations.\"\n",
      "                    },\n",
      "                    \"type\": \"text\"\n",
      "                }\n",
      "            ],\n",
      "            \"created_at\": 1747457265,\n",
      "            \"incomplete_at\": null,\n",
      "            \"incomplete_details\": null,\n",
      "            \"metadata\": {},\n",
      "            \"object\": \"thread.message\",\n",
      "            \"role\": \"assistant\",\n",
      "            \"run_id\": \"run_2Wy2xp5dMoAqVVrLc82nXKEm\",\n",
      "            \"status\": null,\n",
      "            \"thread_id\": \"thread_OJG6UWDKipgOy9WQNwJm07gD\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"msg_H3S270yywv4xgKAHwuh7Iqms\",\n",
      "            \"assistant_id\": \"asst_ILUKBXBTwrrs0TYLNv15mHJF\",\n",
      "            \"attachments\": [],\n",
      "            \"completed_at\": null,\n",
      "            \"content\": [\n",
      "                {\n",
      "                    \"text\": {\n",
      "                        \"annotations\": [],\n",
      "                        \"value\": \"Here is the summary of the Jenkins build information:\\n\\n- **Total builds:** 33\\n\\n**Build Statuses:**\\n- SUCCESS: Count = 24, Percentage = 72.73%\\n- FAILURE: Count = 5, Percentage = 15.15%\\n- UNSTABLE: Count = 1, Percentage = 3.03%\\n- ABORTED: Count = 3, Percentage = 9.09%\\n\\n**Fastest Build:**\\n- Build Label: XYZ-v1_2_0-BUILD_1434\\n- Duration: 1272.56 seconds\\n\\n**Slowest Build:**\\n- Build Label: XYZ-v1_2_0-BUILD_1440\\n- Duration: 42181.094 seconds\\n\\n**Shortest Queue Time:**\\n- Build Label: XYZ-v1_2_0-BUILD_1435\\n- Time: 0.003 seconds\\n\\n**Longest Queue Time:**\\n- Build Label: XYZ-v1_2_0-BUILD_1434\\n- Time: 1803.624 seconds\\n\\n**Average Build Duration:** 13638.579 seconds\\n\\n**Average Queue Time:** 105.429 seconds\"\n",
      "                    },\n",
      "                    \"type\": \"text\"\n",
      "                }\n",
      "            ],\n",
      "            \"created_at\": 1747457286,\n",
      "            \"incomplete_at\": null,\n",
      "            \"incomplete_details\": null,\n",
      "            \"metadata\": {},\n",
      "            \"object\": \"thread.message\",\n",
      "            \"role\": \"assistant\",\n",
      "            \"run_id\": \"run_2Wy2xp5dMoAqVVrLc82nXKEm\",\n",
      "            \"status\": null,\n",
      "            \"thread_id\": \"thread_OJG6UWDKipgOy9WQNwJm07gD\"\n",
      "        }\n",
      "    ],\n",
      "    \"has_more\": false,\n",
      "    \"object\": \"list\",\n",
      "    \"first_id\": \"msg_4xT0bRGNV5terVf9NDIdsllf\",\n",
      "    \"last_id\": \"msg_H3S270yywv4xgKAHwuh7Iqms\"\n",
      "}\n",
      "\n",
      "---------------------\n",
      "\n",
      "\n",
      "------ Message --------\n",
      "\n",
      "Provide Total builds and list all build statuses along their counts and percentages. Also provide the fastest and the slowest build along with their build duration. Also provide the build labels with the longest and shortest queue time. Provide durations too. Also provide the average build and queue duration. \n",
      "\n",
      "---------------------\n",
      "\n",
      "\n",
      "------ Message --------\n",
      "\n",
      "I apologize for the error. There was a type mismatch when calculating the total build duration. It seems like one or more build durations in the JSON data are represented as strings instead of numbers. I will handle this by converting build duration and queue time to integers. Let's try again.\n",
      "\n",
      "---------------------\n",
      "\n",
      "\n",
      "------ Message --------\n",
      "\n",
      "I apologize for the confusion. The build_duration and queue_time seem to be in a time format of 'HH:MM:SS.sss'. Let me convert this into seconds to perform the calculations.\n",
      "\n",
      "---------------------\n",
      "\n",
      "\n",
      "------ Message --------\n",
      "\n",
      "Here is the summary of the Jenkins build information:\n",
      "\n",
      "- **Total builds:** 33\n",
      "\n",
      "**Build Statuses:**\n",
      "- SUCCESS: Count = 24, Percentage = 72.73%\n",
      "- FAILURE: Count = 5, Percentage = 15.15%\n",
      "- UNSTABLE: Count = 1, Percentage = 3.03%\n",
      "- ABORTED: Count = 3, Percentage = 9.09%\n",
      "\n",
      "**Fastest Build:**\n",
      "- Build Label: XYZ-v1_2_0-BUILD_1434\n",
      "- Duration: 1272.56 seconds\n",
      "\n",
      "**Slowest Build:**\n",
      "- Build Label: XYZ-v1_2_0-BUILD_1440\n",
      "- Duration: 42181.094 seconds\n",
      "\n",
      "**Shortest Queue Time:**\n",
      "- Build Label: XYZ-v1_2_0-BUILD_1435\n",
      "- Time: 0.003 seconds\n",
      "\n",
      "**Longest Queue Time:**\n",
      "- Build Label: XYZ-v1_2_0-BUILD_1434\n",
      "- Time: 1803.624 seconds\n",
      "\n",
      "**Average Build Duration:** 13638.579 seconds\n",
      "\n",
      "**Average Queue Time:** 105.429 seconds\n",
      "\n",
      "---------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if run.status == 'completed': \n",
    "    # https://platform.openai.com/docs/api-reference/messages/listMessages\n",
    "    messages = client.beta.threads.messages.list(thread_id=thread.id, order='asc')\n",
    "    print(\"\\n---------------------\\n\")    \n",
    "    print(f\"Thread run completed and returned the following JSON:.\\n\")\n",
    "    print(messages.model_dump_json(indent=4))\n",
    "    print(\"\\n---------------------\\n\")\n",
    "\n",
    "    for message in messages:\n",
    "        text=client.beta.threads.messages.retrieve(message_id=message.id, thread_id=thread.id)\n",
    "        print(\"\\n------ Message --------\\n\")\n",
    "        print(text.content[0].text.value)\n",
    "        print(\"\\n---------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd98791b",
   "metadata": {},
   "source": [
    "## Step 6: Cleanup - delete the original file from the server to free up space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cc6cc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted file, file ID: assistant-THMsYwk8Tfbsj8mgr8uLcv\n"
     ]
    }
   ],
   "source": [
    "client.files.delete(file.id)\n",
    "print(f\"Deleted file, file ID: {file.id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
