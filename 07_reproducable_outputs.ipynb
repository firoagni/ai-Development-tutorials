{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31426544",
   "metadata": {},
   "source": [
    "# Getting Started with Azure OpenAI: Reproducible Outputs\n",
    "\n",
    "By default, responses from the Azure OpenAI chat completion model are nondeterministic - asking the same question multiple times can yield different answers.\n",
    "\n",
    "If you want to obtain reproducible output for the same question, one solution is to set the temperature to 0. However, setting temperature to 0 limits creativity in responses.\n",
    "\n",
    "A better solution for consistent output is to use the optional `seed` parameter.\n",
    "\n",
    "The seed parameter accepts an integer value. When provided, the model makes a \"best effort\" to return the same result for the same parameters and same seed value.\n",
    "\n",
    "**Note:** Determinism isn't 100% guaranteed. Even in cases where the seed value and all other parameters are the same across API calls, it's not uncommon to still observe a degree of variability in responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c1be0",
   "metadata": {},
   "source": [
    "## IMPORTANT NOTE\n",
    "\n",
    "As of September 2025, the `seed` parameter is NOT available in OpenAI's Responses API. It is only supported in the older Chat Completions API.\n",
    "\n",
    "Reference:\n",
    "  - Ticket: https://github.com/openai/openai-node/issues/1613\n",
    "\n",
    "This means you CAN use the `seed` parameter with Azure OpenAI models, but you are restricted to using the Chat Completions API.\n",
    "\n",
    "Related Discussions:\n",
    "  - https://community.openai.com/t/support-for-seed-parameter-in-the-responses-api/1230489/2\n",
    "  - https://community.openai.com/t/seed-parameter-use-in-4-1-and-reasoning-models-too/1298703\n",
    "\n",
    "Because of this limitation, this example uses the Chat Completions API instead of the Responses API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b437222",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Make sure that python3 is installed on your system.\n",
    "2. Create and Activate a Virtual Environment:\n",
    "   - `python3 -m venv venv`\n",
    "   - `source venv/bin/activate`\n",
    "3. The required libraries are listed in the requirements.txt file. Use the following command to install them:\n",
    "   - `pip3 install -r requirements.txt`\n",
    "4. Create a `.env` file in the same directory as this script and add the following variables:\n",
    "   - `AZURE_OPENAI_ENDPOINT=<your_azure_openai_endpoint>`\n",
    "   - `AZURE_OPENAI_MODEL=<your_azure_openai_model>`\n",
    "   - `AZURE_OPENAI_API_VERSION=<your_azure_openai_api_version>`\n",
    "   - `AZURE_OPENAI_API_KEY=<your_azure_openai_api_key>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d279ceca",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "The required libraries are listed in the requirements.txt file. Use the following command to install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "addfcaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 1)) (2.32.4)\n",
      "Requirement already satisfied: openai in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (1.99.9)\n",
      "Requirement already satisfied: dotenv in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (0.9.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (2025.8.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 2)) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 2)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 2)) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 2)) (2.11.7)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 2)) (4.14.1)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv in ./venv/lib/python3.13/site-packages (from dotenv->-r requirements.txt (line 3)) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723fc8c7",
   "metadata": {},
   "source": [
    "## Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2facee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI  # The `AzureOpenAI` library is used to interact with the Azure OpenAI API.\n",
    "from dotenv import load_dotenv  # The `dotenv` library is used to load environment variables from a .env file.\n",
    "import os                       # Used to get the values from environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c5668e",
   "metadata": {},
   "source": [
    "## Load Environment Variables\n",
    "\n",
    "The `load_dotenv()` function reads the .env file and loads the variables as environment variables, making them accessible via `os.environ` or `os.getenv()`.\n",
    "\n",
    "**Note:** \n",
    "- `os.environ[]` raises an exception if the variable is not found\n",
    "- `os.getenv()` does not raise an exception, but returns None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "97ff0ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT        = os.environ['AZURE_OPENAI_ENDPOINT']\n",
    "AZURE_OPENAI_MODEL           = os.environ['AZURE_OPENAI_MODEL']\n",
    "AZURE_OPENAI_API_VERSION     = os.environ['AZURE_OPENAI_VERSION']\n",
    "AZURE_OPENAI_API_KEY         = os.environ['AZURE_OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a25c2",
   "metadata": {},
   "source": [
    "## Create Azure OpenAI Client\n",
    "\n",
    "The `AzureOpenAI` class is part of the `openai` library, which is used to interact with the Azure OpenAI API. It requires the Azure endpoint, API key, and API version to be passed as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f907f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    azure_endpoint = AZURE_OPENAI_ENDPOINT,\n",
    "    api_key = AZURE_OPENAI_API_KEY,  \n",
    "    api_version = AZURE_OPENAI_API_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e47886",
   "metadata": {},
   "source": [
    "## Define the Question\n",
    "\n",
    "Let's define a question to demonstrate the difference between deterministic and non-deterministic responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7fb270c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Question to ask the model: Tell me a short urban legend in 3 lines\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "QUESTION = \"Tell me a short urban legend in 3 lines\"\n",
    "print(\"=\" * 80)\n",
    "print(f\"Question to ask the model: {QUESTION}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e220810",
   "metadata": {},
   "source": [
    "## Other Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dc06d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SYSTEM_PROMPT = \"You are a great storyteller.\"\n",
    "TEMPERATURE = 0.9\n",
    "MAX_TOKENS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca1c757",
   "metadata": {},
   "source": [
    "## Demonstrate Non-deterministic Responses (Without Seed)\n",
    "\n",
    "First, let's generate three responses to the same question without using the seed parameter to show the natural variability in responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f54ecf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Generating 3 responses to the same question without using seed parameter...\n",
      "================================================================================\n",
      "Story Version 1\n",
      "---\n",
      "In a bustling city, a woman receives a text from her best friend saying she's in trouble and needs help. Rushing to the location, she finds her friend's phone lying on the ground, but when she picks it up, a chilling voice whispers from the darkness, \"You shouldn't have come alone.\" The next day, her friend's family receives a message saying she was never seen again.\n",
      "---\n",
      "\n",
      "Story Version 2\n",
      "---\n",
      "In a bustling city, a mysterious woman in red appears at midnight, whispering secrets of the past. Those who listen are said to vanish without a trace by dawn, leaving only a single red rose behind. Locals warn that if you find the rose, it's already too late.\n",
      "---\n",
      "\n",
      "Story Version 3\n",
      "---\n",
      "In a bustling city, a lonely woman would often hear a faint whispering her name in the dark alleys. One night, driven by curiosity, she followed the sound only to find a mirror reflecting her own terrified face, but behind her stood a grinning figure. The next day, she vanished, leaving behind only the echo of her name, still haunting the streets.\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Generating 3 responses to the same question without using seed parameter...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f'Story Version {i + 1}')\n",
    "    print('---')\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_MODEL,\n",
    "            # Note: No seed parameter specified\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": QUESTION}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(response.choices[0].message.content)\n",
    "        print(\"---\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting response: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9d0a70",
   "metadata": {},
   "source": [
    "## Demonstrate Reproducible Responses (With Seed = 42)\n",
    "\n",
    "Now let's generate multiple responses to the same question using the same seed value to show how responses become more consistent and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ac70c99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Generating 3 responses to the same question using seed=42...\n",
      "================================================================================\n",
      "Story Version 1\n",
      "---\n",
      "In a bustling city, a woman always noticed a man in a fedora waiting at the same bus stop. One night, she struck up a conversation and discovered he had died decades ago, buried in the very spot where they stood. The next day, the bus stop was gone, replaced by a small park—his resting place.\n",
      "---\n",
      "\n",
      "Story Version 2\n",
      "---\n",
      "In a bustling city, a woman always noticed a man in a fedora waiting at the same bus stop. One night, she struck up a conversation and discovered he had died decades ago, buried in the very spot where he stood. The next morning, the bus stop was gone, replaced by a small park—his resting place.\n",
      "---\n",
      "\n",
      "Story Version 3\n",
      "---\n",
      "In a bustling city, a woman always noticed a man in a fedora waiting at the same bus stop. One night, she struck up a conversation and discovered he had died decades ago, buried in the very spot where they stood. The next day, the bus stop was gone, replaced by a small park—his resting place.\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Generating 3 responses to the same question using seed=42...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f'Story Version {i + 1}')\n",
    "    print('---')\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_MODEL,\n",
    "            seed=42,  # Setting seed for reproducible outputs\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": QUESTION}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(response.choices[0].message.content)\n",
    "        print(\"---\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting response: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8842f3",
   "metadata": {},
   "source": [
    "## Demonstrate Reproducible Responses (With Seed = 20)\n",
    "\n",
    "Let's try with a different seed value to show how different seeds produce different but consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b58bb373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Generating 3 responses to the same question using seed=20...\n",
      "================================================================================\n",
      "Story Version 1\n",
      "---\n",
      "Late at night, a woman in a white dress appears in the rearview mirror of cars on an empty highway. If you stop to help her, she vanishes, leaving only a chilling coldness behind. The next morning, drivers find their gas tanks mysteriously empty, as if she drained their fuel to escape.\n",
      "---\n",
      "\n",
      "Story Version 2\n",
      "---\n",
      "Late at night, a woman in white stands on the side of a deserted road, hitchhiking. When drivers stop to help, she vanishes from the backseat, leaving only a cold chill behind. The next morning, they find her missing photo on a nearby missing person’s poster—she disappeared decades ago.\n",
      "---\n",
      "\n",
      "Story Version 3\n",
      "---\n",
      "Late at night, a woman in white stands on the side of a deserted road, hitchhiking. When drivers stop to help, she vanishes from the backseat, leaving only a cold chill behind. The next morning, they find her missing photo on a nearby missing person’s poster—she disappeared decades ago.\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Generating 3 responses to the same question using seed=20...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f'Story Version {i + 1}')\n",
    "    print('---')\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_MODEL,\n",
    "            seed=20,  # Setting seed for reproducible outputs\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": QUESTION}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(response.choices[0].message.content)\n",
    "        print(\"---\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting response: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521aace6",
   "metadata": {},
   "source": [
    "## Key Observations\n",
    "\n",
    "1. **Without seed**: Each response will likely be different, showing the natural variability of the model.\n",
    "\n",
    "2. **With seed=42**: The three responses should be very similar or identical, demonstrating reproducibility.\n",
    "\n",
    "3. **With seed=20**: The responses will be consistent among themselves but different from the seed=42 responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
