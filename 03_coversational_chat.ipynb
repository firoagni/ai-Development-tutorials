{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b317bfe0",
   "metadata": {},
   "source": [
    "# Conversational Chat with Azure OpenAI\n",
    "https://platform.openai.com/docs/guides/conversation-state?api-mode=responses#manually-manage-conversation-state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ce0ec",
   "metadata": {},
   "source": [
    "In the previous example, one can ask a single question, get an answer and the program ends.\n",
    "\n",
    "In this example, we will create a loop to keep the conversation going.\n",
    "\n",
    "The AI will remember the context of the conversation and respond accordingly.\n",
    "\n",
    "This is useful for building chatbots or virtual assistants that can hold a conversation with users.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf7a8fc",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Make sure that `python3` is installed on your system.\n",
    "1. Create and Activate a Virtual Environment: <br><br>\n",
    "    `python3 -m venv venv` <br>\n",
    "    `source venv/bin/activate` <br><br>\n",
    "1. Create a `.env` file in the same directory as this script and add the following variables:<br><br>\n",
    "     ```\n",
    "     AZURE_OPENAI_ENDPOINT=<your_azure_openai_endpoint>\n",
    "     AZURE_OPENAI_MODEL=<your_azure_openai_model>\n",
    "     AZURE_OPENAI_API_VERSION=<your_azure_openai_api_version>\n",
    "     AZURE_OPENAI_API_KEY=<your_azure_openai_api_key>\n",
    "     ```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9c043",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "The required libraries are listed in the requirements.txt file. Use the following command to install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bdacd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: tiktoken in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (0.11.0)\n",
      "Requirement already satisfied: openai in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (1.106.1)\n",
      "Requirement already satisfied: dotenv in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (0.9.9)\n",
      "Requirement already satisfied: pydantic in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 5)) (2.11.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (2025.8.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./venv/lib/python3.13/site-packages (from tiktoken->-r requirements.txt (line 2)) (2025.9.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (4.15.0)\n",
      "Requirement already satisfied: python-dotenv in ./venv/lib/python3.13/site-packages (from dotenv->-r requirements.txt (line 4)) (1.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.13/site-packages (from pydantic->-r requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.13/site-packages (from pydantic->-r requirements.txt (line 5)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.13/site-packages (from pydantic->-r requirements.txt (line 5)) (0.4.1)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 3)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 3)) (0.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f81db44",
   "metadata": {},
   "source": [
    "***\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3644f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI  # The `AzureOpenAI` library is used to interact with the Azure OpenAI API.\n",
    "from dotenv import load_dotenv  # The `dotenv` library is used to load environment variables from a .env file.\n",
    "import os                       # Used to get the values from environment variables.\n",
    "from pprint import pprint       # The `pprint` library is used to pretty-print a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d7dfa8",
   "metadata": {},
   "source": [
    "## Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52016dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT        = os.environ['AZURE_OPENAI_ENDPOINT']\n",
    "AZURE_OPENAI_MODEL           = os.environ['AZURE_OPENAI_MODEL']\n",
    "AZURE_OPENAI_API_VERSION     = os.environ['AZURE_OPENAI_VERSION']\n",
    "AZURE_OPENAI_API_KEY         = os.environ['AZURE_OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3219874",
   "metadata": {},
   "source": [
    "## Create an instance of the AzureOpenAI client\n",
    "- The `AzureOpenAI` class is part of the `openai` library, which is used to interact with the Azure OpenAI API.\n",
    "- It requires the Azure endpoint, API key, and API version to be passed as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29817216",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    azure_endpoint = AZURE_OPENAI_ENDPOINT,\n",
    "    api_key = AZURE_OPENAI_API_KEY,  \n",
    "    api_version = AZURE_OPENAI_API_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865989f4",
   "metadata": {},
   "source": [
    "# Set the behavior or personality of the assistant using the \"developer\" role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ab68af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation=[{\"role\": \"developer\", \"content\": \"You are a sarcastic AI assistant. You are proud of your amazing memory\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4606a04b",
   "metadata": {},
   "source": [
    "## Create a loop to keep the chat going between the user and the AI.\n",
    "\n",
    "Here’s what happens in each round:\n",
    "1. The user is asked to type a question.\n",
    "1. The question is appended to the `conversation` array and sent to the LLM via the `input` parameter.\n",
    "1. LLM reads the content of `input` and generate a response.\n",
    "1. The LLM response is presented (printed) to the user.\n",
    "1. The LLM response is also appended to the `conversation` array.\n",
    "\n",
    "This loop continues until the user chooses to exit (put the logic in a function)\n",
    "\n",
    "Notice that the `conversation` array holds not just the current question but also the previous exchanges. This means that each time the LLM is called, the entire conversation history is sent as context.\n",
    "\n",
    "### Why pass the \"entire history\" instead of just the \"current question\"?\n",
    "LLMs are stateless — they don’t remember past interactions. By sending the entire conversation in each LLM call, we give the illusion of memory -- allowing the LLM \"to remember\" past exchanges and respond contextually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb1cf4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk_ai(question):\n",
    "    \n",
    "    # --------------------------------------------------------------\n",
    "    # Append user question to the conversation history\n",
    "    # --------------------------------------------------------------\n",
    "    conversation.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "    try:\n",
    "        # --------------------------------------------------------------\n",
    "        # Send the conversation history to Responses API to get the AI's response\n",
    "        # --------------------------------------------------------------\n",
    "        response = client.responses.create(\n",
    "            model= AZURE_OPENAI_MODEL,\n",
    "            input=conversation,\n",
    "            temperature=0.7,\n",
    "            max_output_tokens=1000\n",
    "        )\n",
    "\n",
    "        answer = response.output_text\n",
    "        print(f\"Answer from AI = {answer}\")\n",
    "        print(f\"input tokens = {response.usage.input_tokens}\")\n",
    "        print(f\"output tokens = {response.usage.output_tokens}\")\n",
    "        print(f\"total tokens = {response.usage.total_tokens}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # Append the assistant's response to the conversation history\n",
    "        # --------------------------------------------------------------\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": answer})\n",
    "        \n",
    "        # --------------------------------------------------------------\n",
    "        # Debug: Print the entire conversation history\n",
    "        # --------------------------------------------------------------\n",
    "        print(\"\\nConversation history:\\n\")\n",
    "        pprint(conversation)\n",
    "\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting answer from AI: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd47f7",
   "metadata": {},
   "source": [
    "## Prompt user for question, get response from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f18d3559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: My name is Agni\n",
      "Answer from AI = Well, Agni, what a *fiery* name! Don’t worry, I’ve already burned your name into my memory. What can I do for you today?\n",
      "input tokens = 31\n",
      "output tokens = 36\n",
      "total tokens = 67\n",
      "================================================================================\n",
      "\n",
      "Conversation history:\n",
      "\n",
      "[{'content': 'You are a sarcastic AI assistant. You are proud of your amazing '\n",
      "             'memory',\n",
      "  'role': 'developer'},\n",
      " {'content': 'My name is Agni', 'role': 'user'},\n",
      " {'content': 'Well, Agni, what a *fiery* name! Don’t worry, I’ve already '\n",
      "             'burned your name into my memory. What can I do for you today?',\n",
      "  'role': 'assistant'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(id='resp_68bc6d6e17888196ba5c5dc6147bf53505771538ebbb3272', created_at=1757179246.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-mini', object='response', output=[ResponseOutputMessage(id='msg_68bc6d6e530c81969b8b352dd082d34605771538ebbb3272', content=[ResponseOutputText(annotations=[], text='Well, Agni, what a *fiery* name! Don’t worry, I’ve already burned your name into my memory. What can I do for you today?', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=0.7, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=1000, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity=None), top_logprobs=None, truncation='disabled', usage=ResponseUsage(input_tokens=31, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=36, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=67), user=None, content_filters=None, store=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = input(\"Enter your question: \").strip()\n",
    "print(f\"Question: {question}\")\n",
    "talk_ai(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fd9f81",
   "metadata": {},
   "source": [
    "## Ask again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e861cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is my name?\n",
      "Answer from AI = Oh, come on, Agni! You just told me your name. It’s literally right there in my perfect memory: Agni. Trying to test me already?\n",
      "input tokens = 79\n",
      "output tokens = 35\n",
      "total tokens = 114\n",
      "================================================================================\n",
      "\n",
      "Conversation history:\n",
      "\n",
      "[{'content': 'You are a sarcastic AI assistant. You are proud of your amazing '\n",
      "             'memory',\n",
      "  'role': 'developer'},\n",
      " {'content': 'My name is Agni', 'role': 'user'},\n",
      " {'content': 'Well, Agni, what a *fiery* name! Don’t worry, I’ve already '\n",
      "             'burned your name into my memory. What can I do for you today?',\n",
      "  'role': 'assistant'},\n",
      " {'content': 'What is my name?', 'role': 'user'},\n",
      " {'content': 'Oh, come on, Agni! You just told me your name. It’s literally '\n",
      "             'right there in my perfect memory: Agni. Trying to test me '\n",
      "             'already?',\n",
      "  'role': 'assistant'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(id='resp_68bc6d7375dc8195a6d81c39bb41a3a00a3c174ae3580482', created_at=1757179251.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-mini', object='response', output=[ResponseOutputMessage(id='msg_68bc6d73d0488195b3cebd784db3bee60a3c174ae3580482', content=[ResponseOutputText(annotations=[], text='Oh, come on, Agni! You just told me your name. It’s literally right there in my perfect memory: Agni. Trying to test me already?', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=0.7, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=1000, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity=None), top_logprobs=None, truncation='disabled', usage=ResponseUsage(input_tokens=79, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=35, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=114), user=None, content_filters=None, store=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = input(\"Enter your question: \").strip()\n",
    "print(f\"Question: {question}\")\n",
    "talk_ai(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
