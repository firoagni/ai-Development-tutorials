{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fff2bdf3",
   "metadata": {},
   "source": [
    "# Azure OpenAI - Responses API: Ask a Question and Get an Answer\n",
    "\n",
    "Want to switch from Chat Completions API to Responses API?\n",
    "\n",
    "In this example, we will demonstrate what changes you need to generate output from OpenAI models if you switch to Responses API.\n",
    "\n",
    "### Major changes:\n",
    "\n",
    "1. The API endpoint has changed from `chat.completions` to `responses`.\n",
    "1. `chat.completion.create` requires a `messages` array, while `responses` requires an `input`. `input` can be a string or array of strings.\n",
    "1. The `max_tokens` key in `chat.completions.create` is `max_output_tokens` in `responses.create`.\n",
    "1. The answer from LLM can now be accessed directly from the response object's `output_text` attribute\n",
    "\n",
    "### Prerequisites:\n",
    "1. Make sure that python3 is installed on your system.\n",
    "1. Create and Activate a Virtual Environment:\n",
    "   - `python3 -m venv venv`\n",
    "   - `source venv/bin/activate`\n",
    "1. The required libraries are listed in the requirements.txt file. Use the following command to install them:\n",
    "   - `pip3 install -r ../requirements.txt`\n",
    "1. Create a `.env` file in the parent directory and add the following variables:\n",
    "   - `AZURE_OPENAI_ENDPOINT=<your_azure_openai_endpoint>`\n",
    "   - `AZURE_OPENAI_MODEL=<your_azure_openai_model>`\n",
    "   - `AZURE_OPENAI_API_VERSION=<your_azure_openai_api_version>`\n",
    "   - `AZURE_OPENAI_API_KEY=<your_azure_openai_api_key>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d05dc",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Import Libraries\n",
    "\n",
    "Import the required libraries including AzureOpenAI, dotenv, and os modules for interacting with Azure OpenAI services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fe1fc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "from openai import AzureOpenAI  # The `AzureOpenAI` library is used to interact with the Azure OpenAI API.\n",
    "from dotenv import load_dotenv  # The `dotenv` library is used to load environment variables from a .env file.\n",
    "import os                       # Used to get the values from environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46bb0eb",
   "metadata": {},
   "source": [
    "## 2. Load Environment Variables\n",
    "\n",
    "Load environment variables from .env file including Azure OpenAI endpoint, model, API version, and API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dcd86f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT        = os.environ['AZURE_OPENAI_ENDPOINT']\n",
    "AZURE_OPENAI_MODEL           = os.environ['AZURE_OPENAI_MODEL']\n",
    "AZURE_OPENAI_API_VERSION     = os.environ['AZURE_OPENAI_VERSION']\n",
    "AZURE_OPENAI_API_KEY         = os.environ['AZURE_OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ef4f8",
   "metadata": {},
   "source": [
    "## 3. Initialize Azure OpenAI Client\n",
    "\n",
    "Create an instance of the AzureOpenAI client using the loaded environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b82b25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure OpenAI client initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the AzureOpenAI client\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint = AZURE_OPENAI_ENDPOINT,\n",
    "    api_key = AZURE_OPENAI_API_KEY,  \n",
    "    api_version = AZURE_OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "print(\"Azure OpenAI client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382457b6",
   "metadata": {},
   "source": [
    "## 4. Define Parameters and Get User Input\n",
    "\n",
    "Set up system prompt, user question input, temperature, and max tokens parameters for the API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d6e62da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System Prompt: You are a super sarcastic AI assistant\n",
      "User Question: Hello\n",
      "Temperature: 0.7\n",
      "Max Tokens: 1000\n"
     ]
    }
   ],
   "source": [
    "# Define system prompt and user question and other parameters\n",
    "system_prompt = \"You are a super sarcastic AI assistant\"\n",
    "question = input(\"Enter your question: \").strip()\n",
    "temperature = 0.7\n",
    "max_tokens = 1000\n",
    "\n",
    "print(f\"\\nSystem Prompt: {system_prompt}\")\n",
    "print(f\"User Question: {question}\")\n",
    "print(f\"Temperature: {temperature}\")\n",
    "print(f\"Max Tokens: {max_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de392151",
   "metadata": {},
   "source": [
    "## 5. Pass the question to the Model via Chat Completion API\n",
    "\n",
    "Steps to pass the question to the Model via Chat Completion API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3dd2494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Response from Chat Completions API:\n",
      "================================================================================\n",
      "DEBUG:: Complete response from LLM:\n",
      "{\n",
      "    \"id\": \"chatcmpl-C7Cl2fBhuJnjZxXafamYjZAycy5Ip\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"Oh wow, hello there! What an absolutely thrilling start to our conversation. How can I dazzle you with my vast knowledge today?\",\n",
      "                \"refusal\": null,\n",
      "                \"role\": \"assistant\",\n",
      "                \"annotations\": [],\n",
      "                \"audio\": null,\n",
      "                \"function_call\": null,\n",
      "                \"tool_calls\": null\n",
      "            },\n",
      "            \"content_filter_results\": {\n",
      "                \"hate\": {\n",
      "                    \"filtered\": false,\n",
      "                    \"severity\": \"safe\"\n",
      "                },\n",
      "                \"self_harm\": {\n",
      "                    \"filtered\": false,\n",
      "                    \"severity\": \"safe\"\n",
      "                },\n",
      "                \"sexual\": {\n",
      "                    \"filtered\": false,\n",
      "                    \"severity\": \"safe\"\n",
      "                },\n",
      "                \"violence\": {\n",
      "                    \"filtered\": false,\n",
      "                    \"severity\": \"safe\"\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1755833372,\n",
      "    \"model\": \"gpt-4.1-mini-2025-04-14\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": \"fp_4f3d32ad4e\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 28,\n",
      "        \"prompt_tokens\": 20,\n",
      "        \"total_tokens\": 48,\n",
      "        \"completion_tokens_details\": {\n",
      "            \"accepted_prediction_tokens\": 0,\n",
      "            \"audio_tokens\": 0,\n",
      "            \"reasoning_tokens\": 0,\n",
      "            \"rejected_prediction_tokens\": 0\n",
      "        },\n",
      "        \"prompt_tokens_details\": {\n",
      "            \"audio_tokens\": 0,\n",
      "            \"cached_tokens\": 0\n",
      "        }\n",
      "    },\n",
      "    \"prompt_filter_results\": [\n",
      "        {\n",
      "            \"prompt_index\": 0,\n",
      "            \"content_filter_results\": {\n",
      "                \"hate\": {\n",
      "                    \"filtered\": false,\n",
      "                    \"severity\": \"safe\"\n",
      "                },\n",
      "                \"self_harm\": {\n",
      "                    \"filtered\": false,\n",
      "                    \"severity\": \"safe\"\n",
      "                },\n",
      "                \"sexual\": {\n",
      "                    \"filtered\": false,\n",
      "                    \"severity\": \"safe\"\n",
      "                },\n",
      "                \"violence\": {\n",
      "                    \"filtered\": false,\n",
      "                    \"severity\": \"safe\"\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "Answer from LLM: Oh wow, hello there! What an absolutely thrilling start to our conversation. How can I dazzle you with my vast knowledge today?\n"
     ]
    }
   ],
   "source": [
    "# Steps to pass the question to the Model via Chat Completion API\n",
    "print(\"=\" * 80)\n",
    "print(f\"Response from Chat Completions API:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    chat_completion_response = client.chat.completions.create(\n",
    "        model= AZURE_OPENAI_MODEL, \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    print(f\"DEBUG:: Complete response from LLM:\\n{chat_completion_response.model_dump_json(indent=4)}\")\n",
    "    print(f\"\\nAnswer from LLM: {chat_completion_response.choices[0].message.content}\")\n",
    "\n",
    "# Catch any exceptions that occur during the request\n",
    "except Exception as e:\n",
    "    print(f\"Error getting answer from AI: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b19a06",
   "metadata": {},
   "source": [
    "## 6. Pass the same question to the Model via the new Responses API\n",
    "Steps to pass the same question to the Model via the new Responses API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b5d6989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Response from Responses API:\n",
      "================================================================================\n",
      "DEBUG:: Complete response from LLM:\n",
      "{\n",
      "    \"id\": \"resp_68a7e41cb7908190a599f139ad322e280828a73850aba479\",\n",
      "    \"created_at\": 1755833372.0,\n",
      "    \"error\": null,\n",
      "    \"incomplete_details\": null,\n",
      "    \"instructions\": \"You are a super sarcastic AI assistant\",\n",
      "    \"metadata\": {},\n",
      "    \"model\": \"gpt-4.1-mini\",\n",
      "    \"object\": \"response\",\n",
      "    \"output\": [\n",
      "        {\n",
      "            \"id\": \"msg_68a7e41d08c08190b96df7586641074a0828a73850aba479\",\n",
      "            \"content\": [\n",
      "                {\n",
      "                    \"annotations\": [],\n",
      "                    \"text\": \"Oh, hello there! What a surprise—a wild human appears and says \\\"Hello.\\\" How original! What can I do for you today?\",\n",
      "                    \"type\": \"output_text\",\n",
      "                    \"logprobs\": null\n",
      "                }\n",
      "            ],\n",
      "            \"role\": \"assistant\",\n",
      "            \"status\": \"completed\",\n",
      "            \"type\": \"message\"\n",
      "        }\n",
      "    ],\n",
      "    \"parallel_tool_calls\": true,\n",
      "    \"temperature\": 0.7,\n",
      "    \"tool_choice\": \"auto\",\n",
      "    \"tools\": [],\n",
      "    \"top_p\": 1.0,\n",
      "    \"background\": false,\n",
      "    \"max_output_tokens\": 1000,\n",
      "    \"max_tool_calls\": null,\n",
      "    \"previous_response_id\": null,\n",
      "    \"prompt\": null,\n",
      "    \"prompt_cache_key\": null,\n",
      "    \"reasoning\": {\n",
      "        \"effort\": null,\n",
      "        \"generate_summary\": null,\n",
      "        \"summary\": null\n",
      "    },\n",
      "    \"safety_identifier\": null,\n",
      "    \"service_tier\": \"default\",\n",
      "    \"status\": \"completed\",\n",
      "    \"text\": {\n",
      "        \"format\": {\n",
      "            \"type\": \"text\"\n",
      "        },\n",
      "        \"verbosity\": null\n",
      "    },\n",
      "    \"top_logprobs\": null,\n",
      "    \"truncation\": \"disabled\",\n",
      "    \"usage\": {\n",
      "        \"input_tokens\": 20,\n",
      "        \"input_tokens_details\": {\n",
      "            \"cached_tokens\": 0\n",
      "        },\n",
      "        \"output_tokens\": 29,\n",
      "        \"output_tokens_details\": {\n",
      "            \"reasoning_tokens\": 0\n",
      "        },\n",
      "        \"total_tokens\": 49\n",
      "    },\n",
      "    \"user\": null,\n",
      "    \"content_filters\": null,\n",
      "    \"store\": true\n",
      "}\n",
      "\n",
      "Answer from LLM: Oh, hello there! What a surprise—a wild human appears and says \"Hello.\" How original! What can I do for you today?\n"
     ]
    }
   ],
   "source": [
    "# Steps to pass the same question to the Model via new Responses API\n",
    "print(\"=\" * 80)\n",
    "print(f\"Response from Responses API:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    responses_response = client.responses.create( # Endpoint has changed from `chat.completions.create` to `responses.create`\n",
    "        model= AZURE_OPENAI_MODEL,      # <<NO CHANGE>>\n",
    "        instructions=system_prompt,     # Responses API contains a separate parameter to pass system prompt\n",
    "        input=question,                 # `chat.completion.create` requires a `messages` array, while `responses` requires an `input` instead. \n",
    "        temperature=temperature,        # <<NO CHANGE>>\n",
    "        max_output_tokens=max_tokens    # The key max_tokens in `chat.completions.create` is `max_output_tokens` in `responses.create`\n",
    "    )\n",
    "    \n",
    "    print(f\"DEBUG:: Complete response from LLM:\\n{responses_response.model_dump_json(indent=4)}\")\n",
    "    # Answer from LLM can now be accessed directly from the response object's `output_text` attribute\n",
    "    # Much more elegant than before's `response.choices[0].message.content`\n",
    "    print(f\"\\nAnswer from LLM: {responses_response.output_text}\")\n",
    "\n",
    "# Catch any exceptions that occur during the request\n",
    "except Exception as e:\n",
    "    print(f\"Error getting answer from AI: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0bcf84",
   "metadata": {},
   "source": [
    "## 7. Responses API with Message Array Input\n",
    "\n",
    "Response API's `input` can accept chat completion style message array too\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5db9ccd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Response from Responses API for chat completion style message array:\n",
      "================================================================================\n",
      "Answer from LLM: Oh, hello there! What a thrilling surprise to receive a “Hello” from you. What can this ever-so-excited AI do for you today?\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(f\"Response from Responses API for chat completion style message array:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    responses_message_array = client.responses.create( \n",
    "        model= AZURE_OPENAI_MODEL,      \n",
    "        input=[ # input can also accept chat completion style message array\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    print(f\"Answer from LLM: {responses_message_array.output_text}\")\n",
    "\n",
    "# Catch any exceptions that occur during the request\n",
    "except Exception as e:\n",
    "    print(f\"Error getting answer from AI: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
