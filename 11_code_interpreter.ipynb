{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf108b94",
   "metadata": {},
   "source": [
    "# Code Interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e896f48",
   "metadata": {},
   "source": [
    "Traditional LLMs are good at generating text, but they struggle with tasks that require math or calculations.\n",
    "\n",
    "**Example:** How many \"r\"s are present in the string \"strawberry\"?  \n",
    "**Answer from LLM:** \"strawberry\" has 2 \"r\"s.\n",
    "\n",
    "**Yikes!** 😅\n",
    "\n",
    "Discussions regarding LLMs can't count:\n",
    "- [Should a custom GPT be able to count the number of items in a JSON list?](https://community.openai.com/t/should-a-custom-gpt-be-able-to-count-the-number-of-items-in-a-json-list/575999)\n",
    "- [Assistant can not search the whole file using file search](https://community.openai.com/t/assistant-can-not-search-the-whole-file-using-file-search/739661/3)\n",
    "- [How do I pass complex and nested large JSON data](https://www.reddit.com/r/OpenAI/comments/15xfcuk/how_do_i_pass_complex_and_nested_large_json_data)\n",
    "\n",
    "To solve this problem, OpenAI has introduced a feature called **\"Code Interpreter\"**.\n",
    "\n",
    "With Code Interpreter enabled:\n",
    "- The LLM will generate **Python code** to solve the problem.\n",
    "- The code is executed in a container.\n",
    "- If the code fails, the LLM automatically debugs and refines it until it executes successfully.\n",
    "- Based on the code execution results, the LLM generates a final answer.\n",
    "\n",
    "By writing Python code, your LLM can solve code, math, and data analysis problems now.\n",
    "\n",
    "### Additional cost of using Code Interpreter  \n",
    "Code Interpreter has additional charges beyond the token based fees for Azure OpenAI usage. Check: [Azure OpenAI Service Pricing](https://azure.microsoft.com/en-gb/pricing/details/cognitive-services/openai-service/)\n",
    "\n",
    "### References\n",
    "\n",
    "- [OpenAI Code Interpreter Documentation](https://platform.openai.com/docs/assistants/tools/code-interpreter)\n",
    "- [Azure AI Foundry - Responses with Code Interpreter](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/responses?tabs=python-key#code-interpreter)\n",
    "- [Azure OpenAI Code Interpreter How-to](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/code-interpreter?tabs=python)\n",
    "- [OpenAI Assistants Quickstart](https://platform.openai.com/docs/assistants/quickstart?example=without-streaming)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113f112d",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Make sure that `python3` is installed on your system.\n",
    "1. Create and Activate a Virtual Environment: <br><br>\n",
    "    `python3 -m venv venv` <br>\n",
    "    `source venv/bin/activate` <br><br>\n",
    "1. Create a `.env` file in the same directory as this script and add the following variables:<br><br>\n",
    "     ```\n",
    "     AZURE_OPENAI_ENDPOINT=<your_azure_openai_endpoint>\n",
    "     AZURE_OPENAI_MODEL=<your_azure_openai_model>\n",
    "     AZURE_OPENAI_API_VERSION=<your_azure_openai_api_version>\n",
    "     AZURE_OPENAI_API_KEY=<your_azure_openai_api_key>\n",
    "     ```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c4a82c",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "The required libraries are listed in the requirements.txt file. Use the following command to install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "409191ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: tiktoken in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (0.11.0)\n",
      "Requirement already satisfied: openai in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (1.108.1)\n",
      "Requirement already satisfied: dotenv in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (0.9.9)\n",
      "Requirement already satisfied: pydantic in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 5)) (2.11.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (2025.8.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./venv/lib/python3.13/site-packages (from tiktoken->-r requirements.txt (line 2)) (2025.9.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.13/site-packages (from pydantic->-r requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.13/site-packages (from pydantic->-r requirements.txt (line 5)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.13/site-packages (from pydantic->-r requirements.txt (line 5)) (0.4.1)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 3)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 3)) (0.16.0)\n",
      "Requirement already satisfied: python-dotenv in ./venv/lib/python3.13/site-packages (from dotenv->-r requirements.txt (line 4)) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c816d0f",
   "metadata": {},
   "source": [
    "***\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e904c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI  # The `AzureOpenAI` library is used to interact with the Azure OpenAI API.\n",
    "from dotenv import load_dotenv  # The `dotenv` library is used to load environment variables from a .env file.\n",
    "import os                       # Used to get the values from environment variables.\n",
    "from pprint import pprint       # The `pprint` library is used to pretty-print a dictionary\n",
    "import json                     # The `json` library is used to work with JSON data in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cfffcb",
   "metadata": {},
   "source": [
    "## Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "164f5cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1976d37f",
   "metadata": {},
   "source": [
    "## Initialize the Azure OpenAI Client\n",
    "\n",
    "We extract the environment variables and store them explicitly to ensure they're available, then initialize the client using these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f498eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract environment variables and store them explicitly to ensure they're available\n",
    "AZURE_OPENAI_ENDPOINT        = os.environ['AZURE_OPENAI_ENDPOINT']\n",
    "AZURE_OPENAI_MODEL           = os.environ['AZURE_OPENAI_MODEL']\n",
    "AZURE_OPENAI_API_VERSION     = os.environ['AZURE_OPENAI_VERSION']\n",
    "AZURE_OPENAI_API_KEY         = os.environ['AZURE_OPENAI_API_KEY']\n",
    "\n",
    "# Initialize the client using the extracted variables\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint = AZURE_OPENAI_ENDPOINT,\n",
    "    api_key = AZURE_OPENAI_API_KEY,  \n",
    "    api_version = AZURE_OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "deployment_name = AZURE_OPENAI_MODEL  # The deployment name of the model to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7304734a",
   "metadata": {},
   "source": [
    "## Load the content of your data file to a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21c9e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"dummy_build_data.json\"  # Path to your local file\n",
    "with open(file_path, 'r', encoding='utf-8') as file: \n",
    "    file_content = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a53c1f",
   "metadata": {},
   "source": [
    "## Container API Limitation in Azure OpenAI\n",
    "\n",
    "⚠️ **Important Note:**\n",
    "\n",
    "OpenAI's Container API for code interpreter is not yet available in the Azure OpenAI SDK. The following approach would be ideal but currently fails:\n",
    "\n",
    "```python\n",
    "# This doesn't work in Azure OpenAI yet:\n",
    "container = client.containers.create(\n",
    "    name=\"container-for-code-interpreter\"\n",
    ")\n",
    "\n",
    "file = client.containers.files.create(\n",
    "    container_id=container.id,      \n",
    "    file=file_content,              \n",
    "    file_id=\"dummy_build_data.json\" \n",
    ")\n",
    "```\n",
    "\n",
    "**Error:** `openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}`\n",
    "\n",
    "**Workaround:**  \n",
    "Pass the file content as input to the LLM instead. This is less than ideal, but works if the file is small enough to fit within the token limit.\n",
    "\n",
    "Hopefully, the Azure team will add container API support soon.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a91b21",
   "metadata": {},
   "source": [
    "## Prepare Developer Message\n",
    "\n",
    "We'll pass the file content as context to the LLM along with instructions for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2254adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_message = f\"\"\"\n",
    "Wrapped within <context> tags is the content of a JSON file you need to analyze.\n",
    "<context>\n",
    "{file_content}\n",
    "</context>\n",
    "\n",
    "# Instructions\n",
    "- The JSON file contains Jenkins build information under the key `results`\n",
    "- Each entry in the `results` array contains information about a build.\n",
    "- Build status of a build can be found by checking the `build_status` key.\n",
    "- Build duration (time build took to complete) can be found by checking the `build_duration` key.\n",
    "- Queue time (time build spent in queue) can be found by checking the `queue_time` key.\n",
    "- Build label can be found by checking the `build_label` key. When somebody ask about a build, make sure to provide the build label.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f4567f",
   "metadata": {},
   "source": [
    "## Send your request to the Azure OpenAI API, this time with Code Interpreter enabled\n",
    "\n",
    "Also note two additional parameters:\n",
    "- `stream=True`: Enables streaming responses, allowing us to see the code execution process in real-time.\n",
    "- `background=True`: Allows the request to run in the background, so you can continue working while waiting for the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a4ef00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = client.responses.create(\n",
    "    model = AZURE_OPENAI_MODEL,\n",
    "    instructions = developer_message,\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Provide Total builds and list all build statuses along their counts and percentages. \"\n",
    "                        \"Also provide the fastest and the slowest build along with their build duration. \"\n",
    "                        \"Also provide the build labels with the longest and shortest queue time. Provide durations too. \"\n",
    "                        \"Also provide the average build and queue duration. \"\n",
    "        }\n",
    "    ],\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"code_interpreter\", # I want to use code interpreter\n",
    "            \"container\": {              # Create a container for the LLM to generate and run Python code\n",
    "                \"type\": \"auto\"          # This approach of auto-creating a container is working in Azure OpenAI\n",
    "            }                           #  while manually creating one using the Container API is not\n",
    "        }\n",
    "    ],\n",
    "    stream=True,     # Its wise to enable streaming for code_interpreter to let users see what's happening behind the scenes\n",
    "    background=True  # Background mode enables you to execute long-running tasks without having to worry about timeouts or other connectivity issues.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d44ab",
   "metadata": {},
   "source": [
    "## Print the chunks as they come in\n",
    "\n",
    "The incoming chunks will also contain LLM's internal monologues related to code generation and interpretation.\n",
    "\n",
    "Apart from the usual chunk types, when code_interpreter is used, you may also see:\n",
    "- `response.code_interpreter_call_code.delta`: LLM is generating code\n",
    "- `response.code_interpreter_call_code.done`: LLM has finished generating code\n",
    "- `response.code_interpreter_call.interpreting`: LLM code is being interpreted\n",
    "- `response.code_interpreter_call.completed`: LLM code interpretation is complete\n",
    "\n",
    "**API Reference:** [Response Streaming with Code Interpreter](https://platform.openai.com/docs/api-reference/responses-streaming/response/code_interpreter_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7008ec46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "AI Analysis Started\n",
      "--------------------------------------------------------------------------------\n",
      "Let's analyze the Jenkins build data to provide the requested information:\n",
      "\n",
      "- Total number of builds\n",
      "- Count and percentage of each build status\n",
      "- Fastest and slowest builds with their build durations\n",
      "- Builds with the longest and shortest queue times with their durations\n",
      "- Average build duration\n",
      "- Average queue time\n",
      "\n",
      "I will calculate these and provide you with the summary.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "from datetime import timedelta\n",
      "import pandas as pd\n",
      "\n",
      "# Extract build data from context JSON\n",
      "builds = [\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1442\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"03:05:03.652\",\n",
      "        \"queue_time\": \"00:00:00.014\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1441\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"04:31:02.521\",\n",
      "        \"queue_time\": \"00:00:00.022\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1440\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"11:43:01.094\",\n",
      "        \"queue_time\": \"00:00:00.029\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1439\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"04:21:00.441\",\n",
      "        \"queue_time\": \"00:00:00.010\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1432\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"03:40:45.313\",\n",
      "        \"queue_time\": \"00:00:00.010\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1431\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"03:55:45.693\",\n",
      "        \"queue_time\": \"00:00:00.011\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1436\",\n",
      "        \"build_status\": \"FAILURE\",\n",
      "        \"build_duration\": \"04:55:46.524\",\n",
      "        \"queue_time\": \"00:00:00.023\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1435\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"03:50:44.503\",\n",
      "        \"queue_time\": \"00:00:00.003\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1434\",\n",
      "        \"build_status\": \"FAILURE\",\n",
      "        \"build_duration\": \"00:21:12.560\",\n",
      "        \"queue_time\": \"00:30:03.624\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1433\",\n",
      "        \"build_status\": \"FAILURE\",\n",
      "        \"build_duration\": \"00:31:22.122\",\n",
      "        \"queue_time\": \"00:00:00.005\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1432\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"04:14:04.119\",\n",
      "        \"queue_time\": \"00:00:00.003\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1431\",\n",
      "        \"build_status\": \"FAILURE\",\n",
      "        \"build_duration\": \"00:40:12.623\",\n",
      "        \"queue_time\": \"00:21:30.024\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1430\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"06:21:36.451\",\n",
      "        \"queue_time\": \"00:00:00.004\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1429\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"04:03:54.192\",\n",
      "        \"queue_time\": \"00:00:00.011\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1422\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"04:43:49.126\",\n",
      "        \"queue_time\": \"00:00:00.020\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1421\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"04:13:52.146\",\n",
      "        \"queue_time\": \"00:00:00.026\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1426\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"03:11:51.195\",\n",
      "        \"queue_time\": \"00:00:00.013\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1425\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"04:11:13.259\",\n",
      "        \"queue_time\": \"00:00:00.022\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1424\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"04:22:33.502\",\n",
      "        \"queue_time\": \"00:00:00.006\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1423\",\n",
      "        \"build_status\": \"FAILURE\",\n",
      "        \"build_duration\": \"01:29:52.351\",\n",
      "        \"queue_time\": \"00:00:00.049\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1422\",\n",
      "        \"build_status\": \"UNSTABLE\",\n",
      "        \"build_duration\": \"03:36:32.504\",\n",
      "        \"queue_time\": \"00:00:00.014\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1421\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"03:31:39.401\",\n",
      "        \"queue_time\": \"00:05:00.392\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1420\",\n",
      "        \"build_status\": \"ABORTED\",\n",
      "        \"build_duration\": \"02:46:29.610\",\n",
      "        \"queue_time\": \"00:00:00.010\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1419\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"03:32:09.062\",\n",
      "        \"queue_time\": \"00:00:00.003\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1412\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"04:06:59.340\",\n",
      "        \"queue_time\": \"00:01:24.511\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1411\",\n",
      "        \"build_status\": \"ABORTED\",\n",
      "        \"build_duration\": \"02:31:31.915\",\n",
      "        \"queue_time\": \"00:00:00.046\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1416\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"03:30:45.642\",\n",
      "        \"queue_time\": \"00:00:00.009\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1415\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"03:52:15.025\",\n",
      "        \"queue_time\": \"00:00:00.012\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1414\",\n",
      "        \"build_status\": \"ABORTED\",\n",
      "        \"build_duration\": \"03:12:40.910\",\n",
      "        \"queue_time\": \"00:00:00.039\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1413\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"04:03:31.626\",\n",
      "        \"queue_time\": \"00:00:00.015\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1412\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"04:12:30.126\",\n",
      "        \"queue_time\": \"00:00:00.112\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1411\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"03:12:32.219\",\n",
      "        \"queue_time\": \"00:00:00.030\"\n",
      "    },\n",
      "    {\n",
      "        \"build_label\": \"XYZ-v1_2_0-BUILD_1410\",\n",
      "        \"build_status\": \"SUCCESS\",\n",
      "        \"build_duration\": \"04:22:52.332\",\n",
      "        \"queue_time\": \"00:00:00.026\"\n",
      "    }\n",
      "]\n",
      "\n",
      "# Helper to convert HH:MM:SS.sss string to timedelta\n",
      "def str_to_timedelta(t_str):\n",
      "    h, m, s = t_str.split(\":\")\n",
      "    seconds, ms = s.split(\".\")\n",
      "    return timedelta(hours=int(h), minutes=int(m), seconds=int(seconds), milliseconds=int(ms))\n",
      "\n",
      "# Create dataframe for analysis\n",
      "df = pd.DataFrame(builds)\n",
      "df['build_duration_td'] = df['build_duration'].apply(str_to_timedelta)\n",
      "df['queue_time_td'] = df['queue_time'].apply(str_to_timedelta)\n",
      "\n",
      "# Total builds\n",
      "total_builds = len(df)\n",
      "\n",
      "# Build statuses and their counts and percentages\n",
      "status_counts = df['build_status'].value_counts()\n",
      "status_percentages = (status_counts / total_builds) * 100\n",
      "\n",
      "# Fastest and slowest build by build duration\n",
      "fastest_build = df.loc[df['build_duration_td'].idxmin()]\n",
      "slowest_build = df.loc[df['build_duration_td'].idxmax()]\n",
      "\n",
      "# Build with longest and shortest queue time\n",
      "longest_queue = df.loc[df['queue_time_td'].idxmax()]\n",
      "shortest_queue = df.loc[df['queue_time_td'].idxmin()]\n",
      "\n",
      "# Average build duration and queue time\n",
      "average_build_duration = df['build_duration_td'].mean()\n",
      "average_queue_time = df['queue_time_td'].mean()\n",
      "\n",
      "(total_builds, status_counts, status_percentages, fastest_build, slowest_build,\n",
      " longest_queue, shortest_queue, average_build_duration, average_queue_time)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Code generation complete.\n",
      "Code is being interpreted...\n",
      "Code interpretation complete ...\n",
      "--------------------------------------------------------------------------------\n",
      "Here is the summary of the Jenkins builds analysis:\n",
      "\n",
      "- Total builds: 33\n",
      "\n",
      "- Build statuses with counts and percentages:\n",
      "  - SUCCESS: 24 (72.73%)\n",
      "  - FAILURE: 5 (15.15%)\n",
      "  - ABORTED: 3 (9.09%)\n",
      "  - UNSTABLE: 1 (3.03%)\n",
      "\n",
      "- Fastest build: \n",
      "  - Build Label: XYZ-v1_2_0-BUILD_1434\n",
      "  - Duration: 00:21:12.560\n",
      "\n",
      "- Slowest build:\n",
      "  - Build Label: XYZ-v1_2_0-BUILD_1440\n",
      "  - Duration: 11:43:01.094\n",
      "\n",
      "- Build with the longest queue time:\n",
      "  - Build Label: XYZ-v1_2_0-BUILD_1434\n",
      "  - Queue Time: 00:30:03.624\n",
      "\n",
      "- Build with the shortest queue time:\n",
      "  - Build Label: XYZ-v1_2_0-BUILD_1435\n",
      "  - Queue Time: 00:00:00.003\n",
      "\n",
      "- Average build duration: 03:47:18.579 (approximately 3 hours 47 minutes)\n",
      "- Average queue time: 00:01:45.429 (approximately 1 minute 45 seconds)\n",
      "\n",
      "If you need any more details or specific data, let me know!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Analysis Complete\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cursor = None\n",
    "for chunk in response:\n",
    "    cursor = chunk.sequence_number # What is cursor? Read the next section to find out\n",
    "    if chunk.type == 'response.created': # LLM has started responding\n",
    "        print(\"-\" * 80)\n",
    "        print(\"AI Analysis Started\")\n",
    "        print(\"-\" * 80)\n",
    "    elif chunk.type == 'response.code_interpreter_call_code.delta': # LLM is generating code in chunks. Keep printing them as they come in\n",
    "        code = chunk.delta\n",
    "        print(code, end='', flush=True)\n",
    "    elif chunk.type == 'response.code_interpreter_call_code.done': # LLM has finished generating code\n",
    "        print(\"\\n\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"Code generation complete.\")\n",
    "    elif chunk.type == 'response.code_interpreter_call.interpreting': # LLM code is being interpreted\n",
    "        print(\"Code is being interpreted...\")\n",
    "    elif chunk.type == 'response.code_interpreter_call.completed': # LLM code interpretation is complete\n",
    "        print(\"Code interpretation complete ...\")\n",
    "        print(\"-\" * 80)\n",
    "    elif chunk.type == 'response.output_text.delta': # LLM is responding in chunks. Keep printing them as they come in\n",
    "        partial_llm_response = chunk.delta\n",
    "        print(partial_llm_response, end='', flush=True)\n",
    "    elif chunk.type == 'response.output_text.done': # LLM response is complete\n",
    "        print(\"\\n\")\n",
    "        print(\"-\" * 80)\n",
    "    elif chunk.type == 'response.completed': # LLM has finished responding\n",
    "        print(\"Analysis Complete\")\n",
    "        print(\"-\" * 80)\n",
    "    elif chunk.type == 'response.error': # Error occurred\n",
    "        print(f\"\\nError from LLM: {chunk.error.message}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ab6a3",
   "metadata": {},
   "source": [
    "## Reconnecting to Background Processes\n",
    "\n",
    "If your connection drops, the response will continue running (thanks to `background=True`) and you can reconnect:\n",
    "\n",
    "```python\n",
    "for chunk in client.responses.stream(resp.id, starting_after=cursor):\n",
    "    print(chunk)\n",
    "```\n",
    "\n",
    "**Note:** The above code snippet to resume the stream is not yet implemented in OpenAI SDK. Keep a tab on this page for updates: [Background Processing Guide](https://platform.openai.com/docs/guides/background)\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
