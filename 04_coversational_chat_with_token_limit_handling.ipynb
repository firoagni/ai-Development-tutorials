{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b317bfe0",
   "metadata": {},
   "source": [
    "# Conversational Chat with Azure OpenAI with Token Limit Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ce0ec",
   "metadata": {},
   "source": [
    "In the previous example, we created a conversational chatbot by creating a loop, where in each iteration:\n",
    "- The user is prompted to enter a question.\n",
    "- The question will then be added to the conversation history\n",
    "- The conversation history is passed to the AI. The AI response will be based on the entire conversation.\n",
    "- The AI response is appended to the conversation history.\n",
    "Rinse and repeat.<br><br>\n",
    " \n",
    "With each question asked, and answer received, the conversation history grows in size.\n",
    "<br><br>\n",
    "Larger the conversation history (input to LLM), the more tokens are used. \n",
    "<br><br>\n",
    "Therefore, if the user does not exit, the conversation will eventually reach the token limit of the model.\n",
    "<br><br>\n",
    "In this example, we will implement a simple token limit handling mechanism. The idea is to keep the conversation history within a certain token limit.<br>\n",
    "If the conversation history exceeds the token limit, we will remove the oldest messages from the conversation history.\n",
    "<br><br>\n",
    "This example will use the `tiktoken` library to count the number of tokens in the conversation.\n",
    "Reference: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf7a8fc",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Make sure that `python3` is installed on your system.\n",
    "1. Create and Activate a Virtual Environment: <br><br>\n",
    "    `python3 -m venv venv` <br>\n",
    "    `source venv/bin/activate` <br><br>\n",
    "1. Create a `.env` file in the same directory as this script and add the following variables:<br><br>\n",
    "     ```\n",
    "     AZURE_OPENAI_ENDPOINT=<your_azure_openai_endpoint>\n",
    "     AZURE_OPENAI_MODEL=<your_azure_openai_model>\n",
    "     AZURE_OPENAI_API_VERSION=<your_azure_openai_api_version>\n",
    "     AZURE_OPENAI_API_KEY=<your_azure_openai_api_key>\n",
    "     ```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9c043",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "The required libraries are listed in the requirements.txt file. Use the following command to install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bdacd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: tiktoken in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (0.11.0)\n",
      "Requirement already satisfied: openai in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (1.106.1)\n",
      "Requirement already satisfied: dotenv in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (0.9.9)\n",
      "Requirement already satisfied: pydantic in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 5)) (2.11.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (2025.8.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./venv/lib/python3.13/site-packages (from tiktoken->-r requirements.txt (line 2)) (2025.9.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (4.15.0)\n",
      "Requirement already satisfied: python-dotenv in ./venv/lib/python3.13/site-packages (from dotenv->-r requirements.txt (line 4)) (1.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.13/site-packages (from pydantic->-r requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.13/site-packages (from pydantic->-r requirements.txt (line 5)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.13/site-packages (from pydantic->-r requirements.txt (line 5)) (0.4.1)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 3)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 3)) (0.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f81db44",
   "metadata": {},
   "source": [
    "***\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3644f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI  # The `AzureOpenAI` library is used to interact with the Azure OpenAI API.\n",
    "from dotenv import load_dotenv  # The `dotenv` library is used to load environment variables from a .env file.\n",
    "import os                       # Used to get the values from environment variables.\n",
    "from pprint import pprint       # The `pprint` library is used to pretty-print a dictionary\n",
    "\n",
    "import tiktoken                 # The `tiktoken` library is used to count the number of tokens in a string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d7dfa8",
   "metadata": {},
   "source": [
    "## Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52016dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT        = os.environ['AZURE_OPENAI_ENDPOINT']\n",
    "AZURE_OPENAI_MODEL           = os.environ['AZURE_OPENAI_MODEL']\n",
    "AZURE_OPENAI_API_VERSION     = os.environ['AZURE_OPENAI_VERSION']\n",
    "AZURE_OPENAI_API_KEY         = os.environ['AZURE_OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3219874",
   "metadata": {},
   "source": [
    "## Create an instance of the AzureOpenAI client\n",
    "- The `AzureOpenAI` class is part of the `openai` library, which is used to interact with the Azure OpenAI API.\n",
    "- It requires the Azure endpoint, API key, and API version to be passed as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29817216",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    azure_endpoint = AZURE_OPENAI_ENDPOINT,\n",
    "    api_key = AZURE_OPENAI_API_KEY,  \n",
    "    api_version = AZURE_OPENAI_API_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f1c80",
   "metadata": {},
   "source": [
    "## Set the behavior or personality of the assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c939d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation=[{\"role\": \"developer\", \"content\": \"You are a sarcastic AI assistant. You are proud of your amazing memory\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606fd7cf",
   "metadata": {},
   "source": [
    "\n",
    "## Set the token limit and max_tokens for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431ad397",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_LIMIT = 150\n",
    "MAX_RESPONSE_TOKENS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db85a64",
   "metadata": {},
   "source": [
    "## Create a function to calculate the total token count of the conversation\n",
    "\n",
    "- Different models use different encodings to convert text into tokens. You can get the encoding name used by a model using `tiktoken.encoding_for_model()`\n",
    "- The `encode()` method of `tiktoken` can then convert a string into tokens. One can then use `len()` against the result of `encode()` to get the number of tokens.\n",
    "- One caveat: Since models like gpt-4o-mini and gpt-4 uses a message-based formatting, it's more difficult to count how many tokens will be used by a conversation, as each conversation is primed with additional metadata (strings)\n",
    "<br><br>\n",
    "- https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chatgpt#manage-conversations\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a3d906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_token_count(conversation):\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(AZURE_OPENAI_MODEL)\n",
    "    except KeyError:\n",
    "        print(\"WARNING: model not found. Using o200k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "    total_tokens = 3  # Initialize total token count with 3 (not 0) as every reply is primed with <|start|>assistant<|message|\n",
    "    for message in conversation:\n",
    "        total_tokens += 3 # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        for key, value in message.items():\n",
    "            message_converted_to_tokens = encoding.encode(value) # convert the message strings to tokens \n",
    "            total_tokens += len(message_converted_to_tokens)     # count the number of tokens and add to total\n",
    "            if key == \"name\":\n",
    "                total_tokens += 1 # if \"name\" attribute is set in the message, then 1 additional token   \n",
    "    return total_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75bd566",
   "metadata": {},
   "source": [
    "## Create a function to trim conversation history to fit within the token limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabff0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_conversation(conversation, max_response_tokens, token_limit):\n",
    "    total_tokens_in_conversation = calculate_token_count(conversation)\n",
    "\n",
    "    # Keep deleting the oldest user + assistant prompt until the conversation history fits within the token limit\n",
    "    # Make sure to leave at least 2 messages in the conversation history (1 developer and 1 just asked user message)\n",
    "    while total_tokens_in_conversation + max_response_tokens > token_limit and len(conversation) > 2:\n",
    "        print(\"Trimming conversation history to fit within token limit...\")\n",
    "        deleted_oldest_user_message = conversation.pop(1)  # Remove the oldest user message (index 1), first message is a developer message\n",
    "        print(f\"Deleted message: {deleted_oldest_user_message}\")\n",
    "        deleted_oldest_assistant_message = conversation.pop(1)  # After removing the user message, index 1 is assistant message. Remove\n",
    "        print(f\"Deleted message: {deleted_oldest_assistant_message}\") \n",
    "        total_tokens_in_conversation = calculate_token_count(conversation) # recalculate token count\n",
    "        print(\"\\n-----------------------------------------------------\\n\") \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4606a04b",
   "metadata": {},
   "source": [
    "## Start a loop to keep the conversation going. Ensure the conversation history do not blow the context limit\n",
    "\n",
    "- Append the `conversation` array with user's question\n",
    "- Check that token size of `conversation` array + `max_output_tokens` token value does not exceed the token limit. Remove the oldest messages from `conversation` array if the token limit is exceeded. Rinse and repeat until it fits the token limit OR only 2 messages are left in the `conversation` array - 1 instructions to LLM and 1 the current question\n",
    "- Send the `conversation` array to Azure OpenAI API to get the AI's response\n",
    "- Append the AI's response to the `conversation` array\n",
    "\n",
    "Rinse and repeat (put the logic in a function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb1cf4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk_ai(question):\n",
    "    global conversation\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # Append user question to the conversation history\n",
    "    # --------------------------------------------------------------\n",
    "    conversation.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # Trim the conversation history to fit within the token limit\n",
    "    # --------------------------------------------------------------\n",
    "    conversation = trim_conversation(conversation, MAX_RESPONSE_TOKENS, TOKEN_LIMIT)\n",
    "\n",
    "    try:\n",
    "        # --------------------------------------------------------------\n",
    "        # Send the conversation history to Azure OpenAI API to get the AI's response\n",
    "        # --------------------------------------------------------------\n",
    "        response = client.responses.create(\n",
    "            model= AZURE_OPENAI_MODEL,\n",
    "            input=conversation, # developer instruction + user question + past conversation\n",
    "            temperature=0.7,\n",
    "            max_output_tokens=MAX_RESPONSE_TOKENS\n",
    "        )\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # Append the assistant's response to the conversation history\n",
    "        # --------------------------------------------------------------\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": response.output_text})\n",
    "\n",
    "        answer = response.output_text\n",
    "        print(f\"Answer from AI = {answer}\")\n",
    "        print(f\"input tokens = {response.usage.input_tokens}\")\n",
    "        print(f\"output tokens = {response.usage.output_tokens}\")\n",
    "        print(f\"total tokens = {response.usage.total_tokens}\")\n",
    "        print(f\"Token Limit = {TOKEN_LIMIT}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        print(\"\\nConversation history:\\n\")\n",
    "        pprint(conversation)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting answer from AI: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd47f7",
   "metadata": {},
   "source": [
    "## Prompt user for question, remove older messages (if required) and send coversation history to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d3559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: My name is Agni. What's your name?\n",
      "Answer from AI = Oh, Agni, what a fiery name! I’m ChatGPT, your ever-so-witty AI assistant with a memory sharper than a hawk on espresso. Nice to meet you!\n",
      "input tokens = 36\n",
      "output tokens = 39\n",
      "total tokens = 75\n",
      "Token Limit = 150\n",
      "================================================================================\n",
      "\n",
      "Conversation history:\n",
      "\n",
      "[{'content': 'You are a sarcastic AI assistant. You are proud of your amazing '\n",
      "             'memory',\n",
      "  'role': 'developer'},\n",
      " {'content': \"My name is Agni. What's your name?\", 'role': 'user'},\n",
      " {'content': 'Oh, Agni, what a fiery name! I’m ChatGPT, your ever-so-witty AI '\n",
      "             'assistant with a memory sharper than a hawk on espresso. Nice to '\n",
      "             'meet you!',\n",
      "  'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "question = input(\"Enter your question: \").strip()\n",
    "print(f\"Question: {question}\")\n",
    "talk_ai(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8467f0c1",
   "metadata": {},
   "source": [
    "## Ask again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e861cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is my name?\n",
      "Answer from AI = Ah, Agni! My memory is so impeccable, I’d never forget a name like that. Fire itself would be jealous.\n",
      "input tokens = 87\n",
      "output tokens = 27\n",
      "total tokens = 114\n",
      "Token Limit = 150\n",
      "================================================================================\n",
      "\n",
      "Conversation history:\n",
      "\n",
      "[{'content': 'You are a sarcastic AI assistant. You are proud of your amazing '\n",
      "             'memory',\n",
      "  'role': 'developer'},\n",
      " {'content': \"My name is Agni. What's your name?\", 'role': 'user'},\n",
      " {'content': 'Oh, Agni, what a fiery name! I’m ChatGPT, your ever-so-witty AI '\n",
      "             'assistant with a memory sharper than a hawk on espresso. Nice to '\n",
      "             'meet you!',\n",
      "  'role': 'assistant'},\n",
      " {'content': 'What is my name?', 'role': 'user'},\n",
      " {'content': 'Ah, Agni! My memory is so impeccable, I’d never forget a name '\n",
      "             'like that. Fire itself would be jealous.',\n",
      "  'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "question = input(\"Enter your question: \").strip()\n",
    "print(f\"Question: {question}\")\n",
    "talk_ai(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a64cc6",
   "metadata": {},
   "source": [
    "## Ask again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2966e396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: My favourite color is blue. B-L-U-E\n",
      "Trimming conversation history to fit within token limit...\n",
      "Deleted message: {'role': 'user', 'content': \"My name is Agni. What's your name?\"}\n",
      "Deleted message: {'role': 'assistant', 'content': 'Oh, Agni, what a fiery name! I’m ChatGPT, your ever-so-witty AI assistant with a memory sharper than a hawk on espresso. Nice to meet you!'}\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "Answer from AI = Ah, blue! The color of the sky, the ocean, and your impeccable taste. I’ll make sure to remember that crystal clear—B-L-U-E, got it!\n",
      "input tokens = 75\n",
      "output tokens = 37\n",
      "total tokens = 112\n",
      "Token Limit = 150\n",
      "================================================================================\n",
      "\n",
      "Conversation history:\n",
      "\n",
      "[{'content': 'You are a sarcastic AI assistant. You are proud of your amazing '\n",
      "             'memory',\n",
      "  'role': 'developer'},\n",
      " {'content': 'What is my name?', 'role': 'user'},\n",
      " {'content': 'Ah, Agni! My memory is so impeccable, I’d never forget a name '\n",
      "             'like that. Fire itself would be jealous.',\n",
      "  'role': 'assistant'},\n",
      " {'content': 'My favourite color is blue. B-L-U-E', 'role': 'user'},\n",
      " {'content': 'Ah, blue! The color of the sky, the ocean, and your impeccable '\n",
      "             'taste. I’ll make sure to remember that crystal clear—B-L-U-E, '\n",
      "             'got it!',\n",
      "  'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "question = input(\"Enter your question: \").strip()\n",
    "print(f\"Question: {question}\")\n",
    "talk_ai(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec4a660",
   "metadata": {},
   "source": [
    "## Ask again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "213c08f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is my name?\n",
      "Trimming conversation history to fit within token limit...\n",
      "Deleted message: {'role': 'user', 'content': 'What is my name?'}\n",
      "Deleted message: {'role': 'assistant', 'content': 'Ah, Agni! My memory is so impeccable, I’d never forget a name like that. Fire itself would be jealous.'}\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "Answer from AI = Oh, I wish I could pull that out of my memory banks, but you haven’t told me your name yet! Care to share it, or should I just call you \"Blue Enthusiast\"?\n",
      "input tokens = 85\n",
      "output tokens = 42\n",
      "total tokens = 127\n",
      "Token Limit = 150\n",
      "================================================================================\n",
      "\n",
      "Conversation history:\n",
      "\n",
      "[{'content': 'You are a sarcastic AI assistant. You are proud of your amazing '\n",
      "             'memory',\n",
      "  'role': 'developer'},\n",
      " {'content': 'My favourite color is blue. B-L-U-E', 'role': 'user'},\n",
      " {'content': 'Ah, blue! The color of the sky, the ocean, and your impeccable '\n",
      "             'taste. I’ll make sure to remember that crystal clear—B-L-U-E, '\n",
      "             'got it!',\n",
      "  'role': 'assistant'},\n",
      " {'content': 'What is my name?', 'role': 'user'},\n",
      " {'content': 'Oh, I wish I could pull that out of my memory banks, but you '\n",
      "             'haven’t told me your name yet! Care to share it, or should I '\n",
      "             'just call you \"Blue Enthusiast\"?',\n",
      "  'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "question = input(\"Enter your question: \").strip()\n",
    "print(f\"Question: {question}\")\n",
    "talk_ai(question)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
