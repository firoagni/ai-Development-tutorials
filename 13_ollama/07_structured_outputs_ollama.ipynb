{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26bdcbfb",
   "metadata": {},
   "source": [
    "# Getting Started with Ollama: Structured Outputs\n",
    "\n",
    "By default, models return responses in plain text format.\n",
    "\n",
    "Structured Outputs is a feature that can force a model to generate responses in JSON format, based on the JSON schema provided by you.\n",
    "\n",
    "Structured Outputs is available in two forms in the OpenAI API:\n",
    "- Function Calling: Demonstrated in next example.\n",
    "- JSON Schema Response Format: Specify a `format` to directly control the structure of the model's output\n",
    "\n",
    "In this demo, we'll focus on using the JSON Schema Response Format.\n",
    "\n",
    "## Steps:\n",
    "1. Define your schema: Write Pydantic classes to define the object schema that represents the structure of the desired output.\n",
    "2. Supply your schema to the API call: Pass the object schema to the model using the `format` parameter.\n",
    "3. Handle edge cases: In some cases, the model might not generate a valid response that matches the provided JSON schema.\n",
    "\n",
    "## Differences from OpenAI responses API\n",
    "1. Instead of `response_format`, Ollama chat API has `format` attribute\n",
    "2. `response_format` accepts the Pydantic class name, while `format` accepts the \"JSON schema\" of the Pydantic Class\n",
    "3. The model output response of Ollama API is plain JSON. To use the output in your python code, you'll need to convert it into the appropriate Pydantic model instance.\n",
    "\n",
    "## Important notes:\n",
    "- Structured output not working with gpt-oss model (Issue: https://github.com/ollama/ollama/issues/11691)\n",
    "- Structured output response is not great with llama3.2:3b model, its a bit better with gemma3:4b and deepseek-r1:8b, but still unreliable\n",
    "- qwen2.5:7b model seems to work best with structured output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88abc7ed",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. Make sure that python3 is installed on your system.\n",
    "2. Make sure Ollama is installed and \"running\" on your system.\n",
    "3. Create an .env file, and add the following line:\n",
    "   ```\n",
    "   OLLAMA_MODEL=<model_name>\n",
    "   ```\n",
    "   model_name will be the name of the local model you want to use\n",
    "4. Create and Activate a Virtual Environment:\n",
    "   ```bash\n",
    "   python3 -m venv venv\n",
    "   source venv/bin/activate\n",
    "   ```\n",
    "5. The required libraries are listed in the requirements.txt file. Use the following command to install them:\n",
    "   ```bash\n",
    "   pip3 install -r requirements.txt\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a685ee",
   "metadata": {},
   "source": [
    "## Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94eefe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the chat API from Ollama (Think of OpenAI chat completion API equivalent)\n",
    "from ollama import chat, ResponseError, pull    \n",
    "\n",
    "# The `dotenv` library is used to load environment variables from a .env file\n",
    "from dotenv import load_dotenv                  \n",
    "\n",
    "# Used to get the values from environment variables\n",
    "import os                                       \n",
    "\n",
    "# Pydantic is used to define the structure of the output we want\n",
    "from pydantic import BaseModel, Field           \n",
    "\n",
    "# Used for type hints in our Pydantic models\n",
    "from typing import List                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eeece6",
   "metadata": {},
   "source": [
    "## Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da37f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "MODEL = os.environ['OLLAMA_MODEL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a11ba25",
   "metadata": {},
   "source": [
    "## Define Output Structure\n",
    "We'll define the output structure we want by writing Pydantic classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dfc583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMConfidence(BaseModel):\n",
    "    confidence: float = Field(description=\"Confidence level in the prediction. \" \\\n",
    "                                    \"Value between 0 lowest to 100 highest.\" \\\n",
    "                                    \"Highest confidence - when all values are clearly mentioned in the input. \" \\\n",
    "                                    \"More the assumptions made by the model, lower the confidence. \"\n",
    "                                    )\n",
    "    confidence_reason: str = Field(description=\"Reasoning behind the confidence level.\")\n",
    "    assumptions: List[str] = Field(description=\"List of assumptions made by the model.\")\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str = Field(description=\"The name of the event\")\n",
    "    date: str = Field(description=\"The date of the event\")\n",
    "    participants: List[str] = Field(description=\"List of participants attending the event\")\n",
    "    \n",
    "    llm_confidence: LLMConfidence = Field(description=\"Confidence information from the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76b7c68",
   "metadata": {},
   "source": [
    "## Define Example Inputs\n",
    "Let's define some example inputs for which we'll generate JSON output in our defined format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5ca6163",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"Mike will attend the Chris Rock Concert on 24 Jan 2025\",\n",
    "    \"Vijay and Venu are going to a science fair on Friday.\",\n",
    "    \"The project deadline is next Monday.\",\n",
    "    \"Vijay and Venu are going to a science fair\",\n",
    "    \"Build Team is planning a team outing first week of August\",\n",
    "    \"Solve 2+2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57181e02",
   "metadata": {},
   "source": [
    "## Process Inputs and Generate Structured Output\n",
    "Now let's process each input and generate structured JSON output using our defined schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1fdad47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Mike will attend the Chris Rock Concert on 24 Jan 2025\n",
      "\n",
      "LLM Response:\n",
      "name='Chris Rock Concert' date='2025-01-24' participants=['Mike'] llm_confidence=LLMConfidence(confidence=1.0, confidence_reason='The input clearly states the event name, date and participant.', assumptions=[])\n",
      "\n",
      "Extracted Event Information:\n",
      "Name: Chris Rock Concert\n",
      "Date: 2025-01-24\n",
      "Participants: Mike\n",
      "Confidence: 1.0\n",
      "Confidence Reason: The input clearly states the event name, date and participant.\n",
      "Assumptions: \n",
      "-------\n",
      "\n",
      "Input: Vijay and Venu are going to a science fair on Friday.\n",
      "\n",
      "LLM Response:\n",
      "name='science fair' date='Friday' participants=['Vijay', 'Venu'] llm_confidence=LLMConfidence(confidence=1.0, confidence_reason='The sentence clearly states the event name and participants.', assumptions=[])\n",
      "\n",
      "Extracted Event Information:\n",
      "Name: science fair\n",
      "Date: Friday\n",
      "Participants: Vijay, Venu\n",
      "Confidence: 1.0\n",
      "Confidence Reason: The sentence clearly states the event name and participants.\n",
      "Assumptions: \n",
      "-------\n",
      "\n",
      "Input: The project deadline is next Monday.\n",
      "\n",
      "LLM Response:\n",
      "name='Project Deadline' date='next Monday' participants=[] llm_confidence=LLMConfidence(confidence=1.0, confidence_reason='Direct statement of a specific event with no ambiguity.', assumptions=[])\n",
      "\n",
      "Extracted Event Information:\n",
      "Name: Project Deadline\n",
      "Date: next Monday\n",
      "Participants: \n",
      "Confidence: 1.0\n",
      "Confidence Reason: Direct statement of a specific event with no ambiguity.\n",
      "Assumptions: \n",
      "-------\n",
      "\n",
      "Input: Vijay and Venu are going to a science fair\n",
      "\n",
      "LLM Response:\n",
      "name='science fair' date='' participants=['Vijay', 'Venu'] llm_confidence=LLMConfidence(confidence=1.0, confidence_reason='The sentence directly states the event and participants involved.', assumptions=[])\n",
      "\n",
      "Extracted Event Information:\n",
      "Name: science fair\n",
      "Date: \n",
      "Participants: Vijay, Venu\n",
      "Confidence: 1.0\n",
      "Confidence Reason: The sentence directly states the event and participants involved.\n",
      "Assumptions: \n",
      "-------\n",
      "\n",
      "Input: Build Team is planning a team outing first week of August\n",
      "\n",
      "LLM Response:\n",
      "name='Team Outing' date='first week of August' participants=['Build Team'] llm_confidence=LLMConfidence(confidence=1.0, confidence_reason='The input clearly states a planned event with specific participants and date.', assumptions=[])\n",
      "\n",
      "Extracted Event Information:\n",
      "Name: Team Outing\n",
      "Date: first week of August\n",
      "Participants: Build Team\n",
      "Confidence: 1.0\n",
      "Confidence Reason: The input clearly states a planned event with specific participants and date.\n",
      "Assumptions: \n",
      "-------\n",
      "\n",
      "Input: Solve 2+2\n",
      "\n",
      "LLM Response:\n",
      "name='' date='' participants=[] llm_confidence=LLMConfidence(confidence=0.0, confidence_reason='The user input does not contain any event information. It is a simple arithmetic problem.', assumptions=[])\n",
      "\n",
      "Extracted Event Information:\n",
      "Name: \n",
      "Date: \n",
      "Participants: \n",
      "Confidence: 0.0\n",
      "Confidence Reason: The user input does not contain any event information. It is a simple arithmetic problem.\n",
      "Assumptions: \n",
      "-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for input in inputs:\n",
    "    print(f\"Input: {input}\")\n",
    "    try:\n",
    "        response = chat(\n",
    "            model = MODEL,\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"Extract the event information from the provided user input\"},\n",
    "                {\"role\": \"user\", \"content\": input}\n",
    "            ],\n",
    "            format = CalendarEvent.model_json_schema(), # Use Pydantic to generate the JSON schema of the Class\n",
    "            options = {\n",
    "                \"temperature\": 0, # Make responses more deterministic\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Extract answer and print it\n",
    "        print(\"\\nLLM Response:\")\n",
    "\n",
    "        # response output is in json format.\n",
    "        response_json = response.message.content\n",
    "\n",
    "        # Use `model_validate_json` class method to convert\n",
    "        # the response JSON into a Pydantic model instance.\n",
    "        calendarEvent = CalendarEvent.model_validate_json(response_json)\n",
    "        print(calendarEvent)\n",
    "        print(\"\\nExtracted Event Information:\")\n",
    "        print(f\"Name: {calendarEvent.name}\")\n",
    "        print(f\"Date: {calendarEvent.date}\")\n",
    "        print(f\"Participants: {', '.join(calendarEvent.participants)}\")\n",
    "        print(f\"Confidence: {calendarEvent.llm_confidence.confidence}\")\n",
    "        print(f\"Confidence Reason: {calendarEvent.llm_confidence.confidence_reason}\")\n",
    "        print(f\"Assumptions: {', '.join(calendarEvent.llm_confidence.assumptions)}\")\n",
    "        print(\"-------\\n\")\n",
    "        \n",
    "    except ResponseError as e:\n",
    "        print('Error getting answer from AI:', e)\n",
    "        if e.status_code == 404: # Model not installed\n",
    "            try:\n",
    "                print('Pulling model:', MODEL)\n",
    "                pull(MODEL) \n",
    "                print('Model pulled successfully:', MODEL)\n",
    "                print('Restart the program again ...')\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Error pulling model. Error:', e)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error getting answer from AI:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd395c5",
   "metadata": {},
   "source": [
    "#### Note \n",
    "Surprizingly, none of model I tried, i.e. llama3.2:3b model, gemma3:4b, deepseek-r1:8b and qwen2.5:7b - respected the instruction \"Value between 0 lowest to 100 highest.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
