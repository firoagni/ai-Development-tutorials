{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "575134c8",
   "metadata": {},
   "source": [
    "# Getting Started with Ollama: Few-Shot Prompting\n",
    "\n",
    "In some cases, it's easier to show the model what you want rather than tell the model what you want.\n",
    "\n",
    "One way to show the model what you want is with creating a few fake back-and-forth messages between user and assistant. This is called few-shot prompting. \n",
    "\n",
    "The opposite of few-shot prompting is zero-shot prompting (previous examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecafb03",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. Make sure that python3 is installed on your system.\n",
    "2. Make sure Ollama is installed and \"running\" on your system.\n",
    "3. Create an .env file, and add the following line:\n",
    "   ```\n",
    "   OLLAMA_MODEL=<model_name>\n",
    "   ```\n",
    "   where model_name will be the name of the local model you want to use\n",
    "4. Create and Activate a Virtual Environment:\n",
    "   ```\n",
    "   python3 -m venv venv\n",
    "   source venv/bin/activate\n",
    "   ```\n",
    "5. The required libraries are listed in the requirements.txt file. Use the following command to install them:\n",
    "   ```\n",
    "   pip3 install -r requirements.txt\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d21d2a",
   "metadata": {},
   "source": [
    "## Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1724219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat, ResponseError, pull    # chat API from Ollama. Think of OpenAI chat completion API equivalent\n",
    "from dotenv import load_dotenv                  # The `dotenv` library is used to load environment variables from a .env file.\n",
    "import os                                       # Used to get the values from environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e822c7",
   "metadata": {},
   "source": [
    "## Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea6705dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "MODEL = os.environ['OLLAMA_MODEL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4568234",
   "metadata": {},
   "source": [
    "## Set the Assistant's Behavior with Few-Shot Examples\n",
    "\n",
    "In this example, we are expecting the assistant to respond in Hindi.\n",
    "The `conversation` list contains a series of messages that simulate such conversation.\n",
    "\n",
    "To help clarify that the example messages are not part of a real conversation, \n",
    "and shouldn't be referred back by the model, \n",
    "set the message role as `system` followed by a `name` field.\n",
    "The value of `name` field can either by `example_user` or `example_assistant`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d86a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation=[\n",
    "        {\"role\": \"system\", \"content\": \"You answer based on the pattern of the conversation.\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Hi, how are you?\"},\n",
    "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Main accha hoon, aap kaise hain?\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"I am fine, can you tell me something?\"},\n",
    "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Haan, bilkul! Aapko kya jaanana hai?\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a935a61",
   "metadata": {},
   "source": [
    "## Start Interactive Chat Loop\n",
    "\n",
    "The loop will continue until you interrupt the kernel.\n",
    "In each iteration:\n",
    "- User will be prompted to enter a question\n",
    "- The question will be added to the conversation history\n",
    "- The AI will respond based on the entire conversation\n",
    "- The conversation history is maintained in the `conversation` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3abe228b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: My name is Agni\n",
      "DEBUG:: Complete response from LLM:\n",
      "{\n",
      "    \"model\": \"qwen2.5:latest\",\n",
      "    \"created_at\": \"2025-08-27T01:28:51.294083Z\",\n",
      "    \"done\": true,\n",
      "    \"done_reason\": \"stop\",\n",
      "    \"total_duration\": 2518867125,\n",
      "    \"load_duration\": 63846041,\n",
      "    \"prompt_eval_count\": 73,\n",
      "    \"prompt_eval_duration\": 115238208,\n",
      "    \"eval_count\": 46,\n",
      "    \"eval_duration\": 2338493083,\n",
      "    \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Achchi nām hai, Agni! Main aapko kyunki main ne apne namse ko zyada rukhana suna hai. Kuchh aur baat kar lenge?\",\n",
      "        \"thinking\": null,\n",
      "        \"images\": null,\n",
      "        \"tool_name\": null,\n",
      "        \"tool_calls\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "Answer from AI:\n",
      "Achchi nām hai, Agni! Main aapko kyunki main ne apne namse ko zyada rukhana suna hai. Kuchh aur baat kar lenge?\n",
      "Question: What is your name?\n",
      "DEBUG:: Complete response from LLM:\n",
      "{\n",
      "    \"model\": \"qwen2.5:latest\",\n",
      "    \"created_at\": \"2025-08-27T01:29:02.77011Z\",\n",
      "    \"done\": true,\n",
      "    \"done_reason\": \"stop\",\n",
      "    \"total_duration\": 1824758958,\n",
      "    \"load_duration\": 67960541,\n",
      "    \"prompt_eval_count\": 133,\n",
      "    \"prompt_eval_duration\": 274600625,\n",
      "    \"eval_count\": 30,\n",
      "    \"eval_duration\": 1479823167,\n",
      "    \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Main System ka naam hoon. Aapki kaise mohabbat ho Agni? Kuch aur baat kar lenge?\",\n",
      "        \"thinking\": null,\n",
      "        \"images\": null,\n",
      "        \"tool_name\": null,\n",
      "        \"tool_calls\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "Answer from AI:\n",
      "Main System ka naam hoon. Aapki kaise mohabbat ho Agni? Kuch aur baat kar lenge?\n",
      "Question: exit\n",
      "Exiting the chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Get user input\n",
    "    question = input(\"Enter your question (exit to quit): \").strip()\n",
    "    print(f\"Question: {question}\")\n",
    "\n",
    "    if question.lower() in ['exit', 'quit']:\n",
    "        print(\"Exiting the chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Add user question to conversation history\n",
    "    conversation.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "    # Wrap the question to ollama.chat() payload\n",
    "    try:\n",
    "        response = chat(\n",
    "            model = MODEL,\n",
    "            messages = conversation,\n",
    "            options = {             \n",
    "                \"temperature\": 0.7,  \n",
    "                \"seed\": 42          \n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Print the response for debugging\n",
    "        print(f\"DEBUG:: Complete response from LLM:\\n{response.model_dump_json(indent=4)}\")\n",
    "\n",
    "        # Extract answer and print it\n",
    "        answer = response.message.content\n",
    "        print(\"\\nAnswer from AI:\")\n",
    "        print(answer)\n",
    "\n",
    "        # Append the assistant's response to the conversation history\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "    # Handle if the provided model is not installed\n",
    "    except ResponseError as e:\n",
    "        print('Error getting answer from AI:', e)\n",
    "        if e.status_code == 404: # Model not installed\n",
    "            try:\n",
    "                print('Pulling model:', MODEL)\n",
    "                pull(MODEL) \n",
    "                print('Model pulled successfully:', MODEL)\n",
    "                print('Please ask the question again.')\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Error pulling model. Error:', e)\n",
    "\n",
    "    # Catch any exceptions that occur during the request\n",
    "    except Exception as e:\n",
    "        print('Error getting answer from AI:', e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
