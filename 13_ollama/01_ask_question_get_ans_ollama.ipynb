{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94825dc4",
   "metadata": {},
   "source": [
    "# Getting Started with Ollama: Ask a Question and Get an Answer\n",
    "\n",
    "This notebook demonstrates how to interact with Ollama via its Python library.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Make sure that python3 is installed on your system.\n",
    "2. Make sure Ollama is installed and \"running\" on your system.\n",
    "3. Create an .env file, and add the following line:\n",
    "   ```\n",
    "   OLLAMA_MODEL=<model_name>\n",
    "   ```\n",
    "   where model_name will be the name of the local model you want to use\n",
    "4. Create and Activate a Virtual Environment:\n",
    "   ```bash\n",
    "   python3 -m venv venv\n",
    "   source venv/bin/activate\n",
    "   ```\n",
    "5. Install required libraries:\n",
    "   ```bash\n",
    "   pip3 install -r requirements.txt\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20814545",
   "metadata": {},
   "source": [
    "## Import Required Modules\n",
    "\n",
    "We'll import the necessary modules for interacting with Ollama and handling environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1c0f8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat, ResponseError, pull    # chat API from Ollama. Think of OpenAI chat completion API equivalent\n",
    "from dotenv import load_dotenv                  # The `dotenv` library is used to load environment variables from a .env file.\n",
    "import os                                       # Used to get the values from environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bba776",
   "metadata": {},
   "source": [
    "## Load Environment Variables\n",
    "\n",
    "Load the model name from the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6240138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "MODEL = os.environ['OLLAMA_MODEL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d3675",
   "metadata": {},
   "source": [
    "## Get User Input\n",
    "\n",
    "Prompt the user for their question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74492729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Hello friend\n"
     ]
    }
   ],
   "source": [
    "question = input(\"Enter your question: \").strip()\n",
    "print(f\"Question: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02e9c97",
   "metadata": {},
   "source": [
    "In Azure OpenAI, you need to create an instance of the AzureOpenAI client first\n",
    "\n",
    "In Ollama, client instance creation step is optional! You can just call ollama.chat() to get the model response "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea0add",
   "metadata": {},
   "source": [
    "## Make API Call to Ollama\n",
    "\n",
    "Send the question to Ollama and handle any potential errors\n",
    "\n",
    "- The `model` parameter specifies the model to be used for the request.\n",
    "- The `messages` array defines the conversation history for the AI model.\n",
    "\n",
    "    Each message includes a `role` and `content`.\n",
    "    `role` specifies the role in the conversation:\n",
    "     - `system`: Sets the behavior or personality of the assistant. The first message in the \"messages\" array\n",
    "    - `user`: Provides the user's input to the model\n",
    "    - `assistant`: Represents the AI's response (used in conversations, check later examples).\n",
    "\n",
    "Additional parameters like `temperature` and `seed` control response behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d3d192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:: Complete response from LLM:\n",
      "{\n",
      "    \"model\": \"qwen2.5:latest\",\n",
      "    \"created_at\": \"2025-08-27T00:55:32.564828Z\",\n",
      "    \"done\": true,\n",
      "    \"done_reason\": \"stop\",\n",
      "    \"total_duration\": 5301013959,\n",
      "    \"load_duration\": 3498969292,\n",
      "    \"prompt_eval_count\": 23,\n",
      "    \"prompt_eval_duration\": 350260083,\n",
      "    \"eval_count\": 29,\n",
      "    \"eval_duration\": 1447198542,\n",
      "    \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Oh, hello there. It's absolutely delightful to meet you. What brings you such joy that you'd even come talk to an AI?\",\n",
      "        \"thinking\": null,\n",
      "        \"images\": null,\n",
      "        \"tool_name\": null,\n",
      "        \"tool_calls\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "Answer from AI:\n",
      "Oh, hello there. It's absolutely delightful to meet you. What brings you such joy that you'd even come talk to an AI?\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = chat(\n",
    "        model = MODEL,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a super sarcastic AI assistant\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        options = {               # See https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\n",
    "            \"temperature\": 0.7,   # Controls the randomness of the output. Lower values make the output more deterministic.\n",
    "            \"seed\": 42           # Setting seed to a specific number will make the model generate the same output for the same input\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # Print the response for debugging\n",
    "    # --------------------------------------------------------------\n",
    "    # The `model_dump_json` method is a custom method provided by the Ollama library to serialize the response object.\n",
    "    # No need to use json.dumps() to convert to a string, as `model_dump_json` already does that.\n",
    "    # The `indent` parameter is used to format the JSON output for better readability.\n",
    "    # ---------------------------------------------------------------\n",
    "    print(f\"DEBUG:: Complete response from LLM:\\n{response.model_dump_json(indent=4)}\")\n",
    "    \n",
    "    # Extract and print the answer\n",
    "    answer = response.message.content\n",
    "    print(\"\\nAnswer from AI:\")\n",
    "    print(answer)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Handle if the provided model is not installed\n",
    "# -------------------------------------------------------------\n",
    "except ResponseError as e:\n",
    "    print('Error getting answer from AI:', e)\n",
    "    if e.status_code == 404: # Model not installed\n",
    "        try:\n",
    "            print('Pulling model:', MODEL)\n",
    "            pull(MODEL) \n",
    "            print('Model pulled successfully:', MODEL)\n",
    "            print('Restart the program again ...')\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Error pulling model. Error:', e)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Catch any exceptions that occur during the request\n",
    "# -------------------------------------------------------------\n",
    "except Exception as e:\n",
    "    print('Error getting answer from AI:', e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
