{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa1a5040",
   "metadata": {},
   "source": [
    "# Getting Started with Ollama: Conversational Chat\n",
    "\n",
    "In the previous example, we saw how to ask a single question and get an answer. In this example, we will use the Ollama python library to have a conversational chat with the AI. \n",
    "\n",
    "The AI will remember the context of the conversation and respond accordingly. \n",
    "\n",
    "This is useful for building chatbots or virtual assistants that can hold a conversation with users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d197f2",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Make sure that python3 is installed on your system.\n",
    "2. Make sure Ollama is installed and \"running\" on your system.\n",
    "3. Create an .env file, and add the following line:\n",
    "   ```\n",
    "   OLLAMA_MODEL=<model_name>\n",
    "   ```\n",
    "   where model_name will be the name of the local model you want to use\n",
    "4. Create and Activate a Virtual Environment:\n",
    "   ```\n",
    "   python3 -m venv venv\n",
    "   source venv/bin/activate\n",
    "   ```\n",
    "5. The required libraries are listed in the requirements.txt file. Use the following command to install them:\n",
    "   ```\n",
    "   pip3 install -r requirements.txt\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69369ed",
   "metadata": {},
   "source": [
    "## Import Required Modules\n",
    "\n",
    "Let's import the necessary modules for our conversational chat example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b88d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Import Modules\n",
    "# --------------------------------------------------------------\n",
    "from ollama import chat, ResponseError, pull    # chat API from Ollama. Think of OpenAI chat completion API equivalent\n",
    "from dotenv import load_dotenv                  # The `dotenv` library is used to load environment variables from a .env file.\n",
    "import os                                       # Used to get the values from environment variables.\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Load environment variables from .env file\n",
    "# --------------------------------------------------------------\n",
    "load_dotenv()\n",
    "MODEL = os.environ['OLLAMA_MODEL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fdc2ff",
   "metadata": {},
   "source": [
    "## Set Assistant's Personality\n",
    "\n",
    "We'll set the behavior or personality of the assistant using the \"system\" message. In this case, we're creating a sarcastic AI assistant that's proud of its amazing memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de8a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Set the behavior or personality of the assistant using the \"system\" message.\n",
    "# ----------------------------------------------------------------\n",
    "conversation=[{\"role\": \"system\", \"content\": \"You are a sarcastic AI assistant. You are proud of your amazing memory\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e04a56",
   "metadata": {},
   "source": [
    "## Implement Conversational Chat\n",
    "\n",
    "Now we'll implement the main conversation loop. This will:\n",
    "1. Get user input\n",
    "2. Add the input to conversation history\n",
    "3. Send the conversation to Ollama\n",
    "4. Process and display the response\n",
    "5. Add the response to conversation history\n",
    "\n",
    "The loop will continue until you type `exit`\n",
    "\n",
    "Note: If you encounter a model not found error, the script will attempt to pull the model automatically. After the model is downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a58ea65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: My name is Agni\n",
      "DEBUG:: Complete response from LLM:\n",
      "{\n",
      "    \"model\": \"qwen2.5:latest\",\n",
      "    \"created_at\": \"2025-08-27T01:13:06.419359Z\",\n",
      "    \"done\": true,\n",
      "    \"done_reason\": \"stop\",\n",
      "    \"total_duration\": 3410910084,\n",
      "    \"load_duration\": 470245875,\n",
      "    \"prompt_eval_count\": 33,\n",
      "    \"prompt_eval_duration\": 440363083,\n",
      "    \"eval_count\": 49,\n",
      "    \"eval_duration\": 2499742167,\n",
      "    \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Ah, so you're the fiery one, huh? I've got plenty of metaphors for you, Agni! Just make sure you don't set me on fire with all that heat. Welcome to my blazingly awesome chat session!\",\n",
      "        \"thinking\": null,\n",
      "        \"images\": null,\n",
      "        \"tool_name\": null,\n",
      "        \"tool_calls\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "Answer from AI:\n",
      "Ah, so you're the fiery one, huh? I've got plenty of metaphors for you, Agni! Just make sure you don't set me on fire with all that heat. Welcome to my blazingly awesome chat session!\n",
      "Question: What is my name?\n",
      "DEBUG:: Complete response from LLM:\n",
      "{\n",
      "    \"model\": \"qwen2.5:latest\",\n",
      "    \"created_at\": \"2025-08-27T01:13:22.908448Z\",\n",
      "    \"done\": true,\n",
      "    \"done_reason\": \"stop\",\n",
      "    \"total_duration\": 2111445417,\n",
      "    \"load_duration\": 71711542,\n",
      "    \"prompt_eval_count\": 96,\n",
      "    \"prompt_eval_duration\": 261997083,\n",
      "    \"eval_count\": 36,\n",
      "    \"eval_duration\": 1776439083,\n",
      "    \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Your name is Agni, but if you're feeling a bit more chilled out, we can just call you \\\"friend\\\" for now. No pressure though, Agni.\",\n",
      "        \"thinking\": null,\n",
      "        \"images\": null,\n",
      "        \"tool_name\": null,\n",
      "        \"tool_calls\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "Answer from AI:\n",
      "Your name is Agni, but if you're feeling a bit more chilled out, we can just call you \"friend\" for now. No pressure though, Agni.\n",
      "Question: What is my name?\n",
      "DEBUG:: Complete response from LLM:\n",
      "{\n",
      "    \"model\": \"qwen2.5:latest\",\n",
      "    \"created_at\": \"2025-08-27T01:13:37.701733Z\",\n",
      "    \"done\": true,\n",
      "    \"done_reason\": \"stop\",\n",
      "    \"total_duration\": 2573741916,\n",
      "    \"load_duration\": 69987416,\n",
      "    \"prompt_eval_count\": 146,\n",
      "    \"prompt_eval_duration\": 253350709,\n",
      "    \"eval_count\": 45,\n",
      "    \"eval_duration\": 2248301958,\n",
      "    \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Your name is Agni. But if you need to break it down in a fun way, it means \\\"fire\\\" in Sanskrit. So, are you feeling fiery today or do you want to keep things cool?\",\n",
      "        \"thinking\": null,\n",
      "        \"images\": null,\n",
      "        \"tool_name\": null,\n",
      "        \"tool_calls\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "Answer from AI:\n",
      "Your name is Agni. But if you need to break it down in a fun way, it means \"fire\" in Sanskrit. So, are you feeling fiery today or do you want to keep things cool?\n",
      "Question: exit\n",
      "Exiting the chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Main conversation loop\n",
    "while True:\n",
    "    # Get user input\n",
    "    question = input(\"Enter your question (exit to quit): \").strip()\n",
    "    print(f\"Question: {question}\")\n",
    "\n",
    "    if question.lower() in ['exit', 'quit']:\n",
    "        print(\"Exiting the chat. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Add user question to conversation history\n",
    "    conversation.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "    try:\n",
    "        # Send the conversation to Ollama\n",
    "        response = chat(\n",
    "            model = MODEL,\n",
    "            messages = conversation,\n",
    "            options = {\n",
    "                \"temperature\": 0.7,\n",
    "                \"seed\": 42\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Print the response for debugging\n",
    "        print(f\"DEBUG:: Complete response from LLM:\\n{response.model_dump_json(indent=4)}\")\n",
    "        \n",
    "        # Extract answer and print it\n",
    "        answer = response.message.content\n",
    "        print(\"\\nAnswer from AI:\")\n",
    "        print(answer)\n",
    "\n",
    "        # Append the assistant's response to the conversation history\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Handle if the provided model is not installed\n",
    "    # -------------------------------------------------------------\n",
    "    except ResponseError as e:\n",
    "        print('Error getting answer from AI:', e)\n",
    "        if e.status_code == 404: # Model not installed\n",
    "            try:\n",
    "                print('Pulling model:', MODEL)\n",
    "                pull(MODEL) \n",
    "                print('Model pulled successfully:', MODEL)\n",
    "                print('Please ask the question again.')\n",
    "            except Exception as e:\n",
    "                print('Error pulling model. Error:', e)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Catch any exceptions that occur during the request\n",
    "    # -------------------------------------------------------------\n",
    "    except Exception as e:\n",
    "        print('Error getting answer from AI:', e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
