{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d66cc2ae",
   "metadata": {},
   "source": [
    "# Getting Started with Ollama: Remote Ollama\n",
    "\n",
    "All previous tutorials are written assuming Ollama is running on the same machine where the script is executed. \n",
    "\n",
    "In this tutorial, we will cover how to interact with a remote instance of Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9e731e",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "1. Make sure that python3 is installed on your system.\n",
    "2. Make sure remote Ollama is accessible (via HTTP) from the system where the script is executed.\n",
    "3. Create an .env file, and add the following line:\n",
    "   \n",
    "   ```\n",
    "   OLLAMA_MODEL=<model_name>\n",
    "   ```\n",
    "   where model_name will be the name of the model you want to use\n",
    "4. Create and Activate a Virtual Environment:\n",
    "   ```bash\n",
    "   python3 -m venv venv\n",
    "   source venv/bin/activate\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f20c64",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "The required libraries are listed in the requirements.txt file. Use the following command to install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c85cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (0.5.3)\n",
      "Requirement already satisfied: httpx>=0.27 in ./venv/lib/python3.9/site-packages (from ollama->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in ./venv/lib/python3.9/site-packages (from ollama->-r requirements.txt (line 1)) (2.11.7)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.9/site-packages (from httpx>=0.27->ollama->-r requirements.txt (line 1)) (4.10.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.9/site-packages (from httpx>=0.27->ollama->-r requirements.txt (line 1)) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.9/site-packages (from httpx>=0.27->ollama->-r requirements.txt (line 1)) (1.0.9)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.9/site-packages (from httpx>=0.27->ollama->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.9/site-packages (from httpcore==1.*->httpx>=0.27->ollama->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.9/site-packages (from pydantic>=2.9->ollama->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.9/site-packages (from pydantic>=2.9->ollama->-r requirements.txt (line 1)) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./venv/lib/python3.9/site-packages (from pydantic>=2.9->ollama->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.9/site-packages (from pydantic>=2.9->ollama->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./venv/lib/python3.9/site-packages (from anyio->httpx>=0.27->ollama->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.9/site-packages (from anyio->httpx>=0.27->ollama->-r requirements.txt (line 1)) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3fbb97",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a0e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client, chat, ResponseError, pull  # chat API from Ollama. Think of OpenAI chat completion API equivalent\n",
    "from dotenv import load_dotenv                        # The `dotenv` library is used to load environment variables from a .env file.\n",
    "import os                                             # Used to get the values from environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd8b848",
   "metadata": {},
   "source": [
    "## Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "304bdf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "MODEL = os.environ['OLLAMA_MODEL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bfb833",
   "metadata": {},
   "source": [
    "## Prompt user for question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65def26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input(\"Enter your question: \").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857145e6",
   "metadata": {},
   "source": [
    "## Initialize a custom Ollama Client\n",
    "To create a connection to Ollama hosted to a remote server, instantiate a custom client by passing the Ollama host URL as argument\n",
    "\n",
    "Argument list: https://www.python-httpx.org/api/#client\n",
    "\n",
    "<<<< This is the only change you require to switch from local to remote Ollama  >>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "787d6a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\n",
    "    host='http://localhost:11434',\n",
    "    # headers={'Authorization': (os.getenv('OLLAMA_API_KEY'))}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ff850e",
   "metadata": {},
   "source": [
    "## Wrap the question to client.chat() payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "077ac116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:: Complete response from LLM:\n",
      "{\n",
      "    \"model\": \"qwen2.5:latest\",\n",
      "    \"created_at\": \"2025-08-27T10:51:30.883131Z\",\n",
      "    \"done\": true,\n",
      "    \"done_reason\": \"stop\",\n",
      "    \"total_duration\": 3788554792,\n",
      "    \"load_duration\": 979205000,\n",
      "    \"prompt_eval_count\": 27,\n",
      "    \"prompt_eval_duration\": 364920291,\n",
      "    \"eval_count\": 47,\n",
      "    \"eval_duration\": 2443559625,\n",
      "    \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Oh, hello there! I'm just a quantum of sarcasm and wit, so I'm feeling pretty snarky today. How about you? Are you having as much fun as I am pretending to be your AI friend?\",\n",
      "        \"thinking\": null,\n",
      "        \"images\": null,\n",
      "        \"tool_name\": null,\n",
      "        \"tool_calls\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "Answer from AI:\n",
      "Oh, hello there! I'm just a quantum of sarcasm and wit, so I'm feeling pretty snarky today. How about you? Are you having as much fun as I am pretending to be your AI friend?\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = client.chat(\n",
    "        model = MODEL,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a super sarcastic AI assistant\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        options = {\n",
    "            \"temperature\": 0.7,  \n",
    "            \"seed\": 42           \n",
    "        }\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # Print the response for debugging\n",
    "    # --------------------------------------------------------------\n",
    "    print(f\"DEBUG:: Complete response from LLM:\\n{response.model_dump_json(indent=4)}\")\n",
    "    \n",
    "    # --------------------------------------------------------------\n",
    "    # Extract answer and print it\n",
    "    # --------------------------------------------------------------\n",
    "    answer = response.message.content\n",
    "    print(\"\\nAnswer from AI:\")\n",
    "    print(answer)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Handle if the provided model is not installed\n",
    "# -------------------------------------------------------------\n",
    "except ResponseError as e:\n",
    "    print('Error getting answer from AI:', e)\n",
    "    if e.status_code == 404: # Model not installed\n",
    "        try:\n",
    "            print('Pulling model:', MODEL)\n",
    "            pull(MODEL) \n",
    "            print('Model pulled successfully:', MODEL)\n",
    "            print('Restart the program again ...')\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Error pulling model. Error:', e)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Catch any exceptions that occur during the request\n",
    "# -------------------------------------------------------------\n",
    "except Exception as e:\n",
    "    print('Error getting answer from AI:', e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
